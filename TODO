Benchmark on GSM8k and ARC/ETHICS Dataset
Work on Wrapper stuff -> barebones structure


Figure out how to wrap so that attention patterns for models are figured.FloatingPointError
    
Maybe have a pipeline:
1. load model into contextQ,
2. let it analyze the model and specific layers,
3. auto adjust based on attention patterns(do this only for llama so far)

Build the wrapper and learn.
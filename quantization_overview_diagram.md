# Quantization Analysis Overview

## 4-bit vs 2-bit Strategic Analysis

```mermaid
graph TD
    subgraph QUANT ["üéØ 4-BIT vs 2-BIT QUANTIZATION ANALYSIS"]
        subgraph DATASETS ["üìä Dataset Performance"]
            ARC4["ARC Dataset<br/>4-bit: 5.0% accuracy<br/>1/20 correct"] 
            ARC2["ARC Dataset<br/>2-bit: 5.0% accuracy<br/>1/20 correct"]
            SVAMP4["SVAMP Dataset<br/>4-bit: 5.0% accuracy<br/>1/20 correct"]
            SVAMP2["SVAMP Dataset<br/>2-bit: 5.0% accuracy<br/>1/20 correct"]
        end
        
        subgraph LAYERS ["üß† Layer Criticality Analysis"]
            L0["Layer 0<br/>Sparsity: 15.3%<br/>‚ùå Non-Critical"]
            L8["Layer 8<br/>Sparsity: 4.6%<br/>‚ùå Non-Critical"] 
            L16["Layer 16<br/>Sparsity: 15.5%<br/>üî¥ CRITICAL"]
            L24["Layer 24<br/>Sparsity: 6.7%<br/>üî¥ CRITICAL"]
            L31["Layer 31<br/>Sparsity: 11.3%<br/>üî¥ CRITICAL"]
        end
        
        subgraph ATTENTION ["üéØ Attention Pattern Analysis"]
            ATT8["Layer 8 Attention<br/>Entropy: 2.95<br/>Head Spec: 50.8%<br/>Reasoning: 57.7%"]
            ATT16["Layer 16 Attention<br/>Entropy: 0.38<br/>Head Spec: 89.8%<br/>Reasoning: 46.1%"]
            ATT24["Layer 24 Attention<br/>Entropy: 1.03<br/>Head Spec: 76.3%<br/>Reasoning: 27.1%"]
            ATT31["Layer 31 Attention<br/>Entropy: 0.96<br/>Head Spec: 85.3%<br/>Reasoning: 77.7%"]
        end
        
        subgraph EFFECTS ["‚ö° Quantization Effects"]
            EARLY["Early Layers (0-15)<br/>‚úÖ Safe for 2-bit<br/>Low impact on performance"]
            MID["Middle Layers (16-24)<br/>‚ö†Ô∏è Mixed precision needed<br/>Critical for reasoning"]
            LATE["Late Layers (24-31)<br/>üî¥ Preserve 4-bit<br/>High reasoning sensitivity"]
        end
        
        subgraph RECOMMENDATIONS ["üí° Optimization Strategy"]
            MIXED["Mixed Precision Strategy"]
            SAFE["Layers 0-15: 2-bit safe"]
            PRESERVE["Layers 16-31: Keep 4-bit"]
            MATH["Math tasks: Higher precision"]
        end
    end
    
    %% Connections
    ARC4 -.-> L16
    ARC2 -.-> L24
    SVAMP4 -.-> L31
    SVAMP2 -.-> ATT31
    
    L0 --> EARLY
    L8 --> EARLY
    L16 --> MID
    L24 --> MID
    L31 --> LATE
    
    ATT8 --> EARLY
    ATT16 --> MID
    ATT24 --> MID
    ATT31 --> LATE
    
    EARLY --> SAFE
    MID --> MIXED
    LATE --> PRESERVE
    
    %% Styling
    classDef critical fill:#ff6b6b,stroke:#d63031,stroke-width:2px,color:#fff
    classDef safe fill:#00b894,stroke:#00a085,stroke-width:2px,color:#fff
    classDef warning fill:#fdcb6e,stroke:#e17055,stroke-width:2px,color:#333
    classDef attention fill:#74b9ff,stroke:#0984e3,stroke-width:2px,color:#fff
    classDef recommendation fill:#a29bfe,stroke:#6c5ce7,stroke-width:2px,color:#fff
    
    class L16,L24,L31,LATE critical
    class L0,L8,EARLY,SAFE safe
    class MID,MIXED warning
    class ATT8,ATT16,ATT24,ATT31 attention
    class MIXED,SAFE,PRESERVE,MATH recommendation
```

## Key Findings

**Critical Layers Identified:**
- üî¥ Layer 16: 15.5% sparsity, critical for reasoning
- üî¥ Layer 24: 6.7% sparsity, high attention specialization  
- üî¥ Layer 31: 11.3% sparsity, highest reasoning score (77.7%)

**Optimization Strategy:**
- ‚úÖ **Layers 0-15**: Safe for 2-bit quantization
- ‚ö†Ô∏è **Layers 16-24**: Mixed precision recommended
- üî¥ **Layers 24-31**: Preserve 4-bit for optimal performance 
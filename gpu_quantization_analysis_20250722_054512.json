{
  "ARC_4bit": {
    "dataset": "ARC",
    "bits": 4,
    "accuracy": 0.05,
    "correct": 1,
    "total": 20,
    "time": 0.0010318756103515625,
    "device": "cpu",
    "critical_layers_affected": [],
    "layer_analysis_summary": {
      "0": {
        "sparsity": 0.152711671030213,
        "critical": false
      },
      "8": {
        "sparsity": 0.04630632478709914,
        "critical": false
      },
      "16": {
        "sparsity": 0.15489371171996494,
        "critical": true
      },
      "24": {
        "sparsity": 0.06729615808970743,
        "critical": true
      },
      "31": {
        "sparsity": 0.11275057922957064,
        "critical": true
      }
    },
    "attention_summary": {
      "8": {
        "entropy": 2.946904802106911,
        "head_specialization": 0.5080503367784424,
        "reasoning_score": 0.5766916557103096
      },
      "16": {
        "entropy": 0.37701896556876635,
        "head_specialization": 0.8978766723178682,
        "reasoning_score": 0.46140137216126675
      },
      "24": {
        "entropy": 1.0272208358197819,
        "head_specialization": 0.7625634836050225,
        "reasoning_score": 0.27083103261428376
      },
      "31": {
        "entropy": 0.961059653129765,
        "head_specialization": 0.8532590202428622,
        "reasoning_score": 0.7774671322454624
      }
    },
    "detailed_results": [
      {
        "sample": 0,
        "question": "Which factor will most likely cause a person to develop a fever?",
        "expected": "B",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.09570889365135049,
            "activation_std": 0.09260611195080698,
            "sparsity": 0.1724288811486891,
            "magnitude": 0.6761274697350965,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.022862328944025487,
            "activation_std": 0.03653076916738831,
            "sparsity": 0.22661849984545165,
            "magnitude": 1.3013495475586674,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.13719126844268834,
            "activation_std": 0.034228993974323274,
            "sparsity": 0.39022010009051644,
            "magnitude": 0.15410058549631384,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.09138182382947174,
            "activation_std": 0.09487306278420321,
            "sparsity": 0.24179372444230054,
            "magnitude": 0.791715111645686,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.019184968493940333,
            "activation_std": 0.09742725846353675,
            "sparsity": 0.08308972004798248,
            "magnitude": 4.672958745967396,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.14376434300511837,
            "head_specialization": 0.7803015644541168,
            "reasoning_score": 0.6524434648892374
          },
          "16": {
            "entropy": 1.065635564405178,
            "head_specialization": 0.8299341627851405,
            "reasoning_score": 0.45672268549653916
          },
          "24": {
            "entropy": 2.0895461599111878,
            "head_specialization": 0.7892612133687901,
            "reasoning_score": 0.708604844837639
          },
          "31": {
            "entropy": 0.8313301177274746,
            "head_specialization": 0.6952767396772954,
            "reasoning_score": 0.7369058710956737
          }
        }
      },
      {
        "sample": 1,
        "question": "Lichens are symbiotic organisms made of green algae and fungi. What do the green algae supply to the...",
        "expected": "B",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.11750654565595114,
            "activation_std": 0.04770481984897401,
            "sparsity": 0.4824929127262626,
            "magnitude": 0.15535348179383626,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.07411827583152124,
            "activation_std": 0.0885741506152995,
            "sparsity": 0.05723555015484933,
            "magnitude": 0.11311091098889028,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.11831121032863286,
            "activation_std": 0.04206966995595238,
            "sparsity": 0.18354334995914356,
            "magnitude": 0.27423521036582565,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.1576333273932139,
            "activation_std": 0.1858045715092813,
            "sparsity": 0.14636805351441354,
            "magnitude": 2.607352365295256,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.04621912449676795,
            "activation_std": 0.0982190683087289,
            "sparsity": 0.10508693494100795,
            "magnitude": 4.276613470483549,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.1691850761234244,
            "head_specialization": 0.6279956650109102,
            "reasoning_score": 0.7692391961442769
          },
          "16": {
            "entropy": 0.3381394968916852,
            "head_specialization": 0.6373810002664295,
            "reasoning_score": 0.6957969315456453
          },
          "24": {
            "entropy": 0.8192658964957126,
            "head_specialization": 0.9072533564695819,
            "reasoning_score": 0.3754434690772801
          },
          "31": {
            "entropy": 0.6514330958867032,
            "head_specialization": 0.6672287767011873,
            "reasoning_score": 0.15514218935369362
          }
        }
      },
      {
        "sample": 2,
        "question": "When a switch is used in an electrical circuit, the switch can",
        "expected": "D",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.07645116082738741,
            "activation_std": 0.052749493919786596,
            "sparsity": 0.13182933875358882,
            "magnitude": 0.49334145824715475,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.13421579669081907,
            "activation_std": 0.04505165515950399,
            "sparsity": 0.11345010016506168,
            "magnitude": 1.198609320899545,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.08073572634250024,
            "activation_std": 0.10733963583999917,
            "sparsity": 0.2061208850337722,
            "magnitude": 0.47686598641016925,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.14046724022590895,
            "activation_std": 0.02414107006689265,
            "sparsity": 0.2589982668656931,
            "magnitude": 7.225162675733486,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.060069682623865805,
            "activation_std": 0.1719749283706697,
            "sparsity": 0.27747648926915286,
            "magnitude": 0.17873589030634632,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.5720543472348321,
            "head_specialization": 0.44408195504095377,
            "reasoning_score": 0.6189193014063
          },
          "16": {
            "entropy": 3.599823804114305,
            "head_specialization": 0.510240226940345,
            "reasoning_score": 0.6214611653814751
          },
          "24": {
            "entropy": 1.690704484113959,
            "head_specialization": 0.9479847016016022,
            "reasoning_score": 0.4826953699956982
          },
          "31": {
            "entropy": 0.4309611118071587,
            "head_specialization": 0.8567324448191013,
            "reasoning_score": 0.9169037515002519
          }
        }
      },
      {
        "sample": 3,
        "question": "Which of the following is an example of an assistive device?",
        "expected": "A",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.059378971659892636,
            "activation_std": 0.041053913635929,
            "sparsity": 0.27690692006841283,
            "magnitude": 0.9545301043996646,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.0459974119309192,
            "activation_std": 0.078646067907092,
            "sparsity": 0.0729415158126688,
            "magnitude": 0.45314835996778957,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.09951557826951911,
            "activation_std": 0.03302617562175735,
            "sparsity": 0.0482870780070419,
            "magnitude": 0.6566711444033312,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.08441590251164927,
            "activation_std": 0.0708556586747106,
            "sparsity": 0.1392445892212296,
            "magnitude": 5.179201446258955,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.012646409649385897,
            "activation_std": 0.13194900844323906,
            "sparsity": 0.39367960412535513,
            "magnitude": 2.4024827794279155,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.2905228057218037,
            "head_specialization": 0.9759344639764242,
            "reasoning_score": 0.5146048286489844
          },
          "16": {
            "entropy": 0.9831924952959172,
            "head_specialization": 0.5600449546624655,
            "reasoning_score": 0.7105359164059665
          },
          "24": {
            "entropy": 1.8287340334080728,
            "head_specialization": 0.5043593263468508,
            "reasoning_score": 0.6320977378788241
          },
          "31": {
            "entropy": 0.7828641313149403,
            "head_specialization": 0.8063095754632629,
            "reasoning_score": 0.47774024032476875
          }
        }
      },
      {
        "sample": 4,
        "question": "Rocks are classified as igneous, metamorphic, or sedimentary according to",
        "expected": "3",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.12647236209568255,
            "activation_std": 0.2928640507171907,
            "sparsity": 0.12582067969668817,
            "magnitude": 0.5629379614501605,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.05405277875682214,
            "activation_std": 0.14788722193588882,
            "sparsity": 0.11361558145171669,
            "magnitude": 3.2506649364985902,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.10343041570780452,
            "activation_std": 0.07728341455093993,
            "sparsity": 0.2906680478850728,
            "magnitude": 2.4440673550011516,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.09244896468083322,
            "activation_std": 0.10680869180732927,
            "sparsity": 0.18089633171108618,
            "magnitude": 2.4514549099677576,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.15838433083471773,
            "activation_std": 0.10162899571710789,
            "sparsity": 0.37818863520897794,
            "magnitude": 1.374772710129627,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.9490483394726599,
            "head_specialization": 0.749679022370291,
            "reasoning_score": 0.8998904976220102
          },
          "16": {
            "entropy": 1.589611609533177,
            "head_specialization": 0.7318123312540064,
            "reasoning_score": 0.5042029468980204
          },
          "24": {
            "entropy": 2.5865850764890777,
            "head_specialization": 0.8083672471430212,
            "reasoning_score": 0.5326488435729202
          },
          "31": {
            "entropy": 1.5532961116010169,
            "head_specialization": 0.9029441931885636,
            "reasoning_score": 0.33520303809746405
          }
        }
      },
      {
        "sample": 5,
        "question": "A chewable calcium carbonate tablet is a common treatment for stomach discomfort. Calcium carbonate ...",
        "expected": "C",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.161327311395962,
            "activation_std": 0.058592166388208404,
            "sparsity": 0.02786708740267334,
            "magnitude": 2.9546824485433563,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.08474373197756926,
            "activation_std": 0.17591943393047849,
            "sparsity": 0.29878569771367375,
            "magnitude": 1.146914460394599,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.13063418496422244,
            "activation_std": 0.07147577709742886,
            "sparsity": 0.0692920124161739,
            "magnitude": 1.7818162402928026,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.013010149840931906,
            "activation_std": 0.020698546123739715,
            "sparsity": 0.041194995753728425,
            "magnitude": 2.4130956141413886,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.028598902673047016,
            "activation_std": 0.10197134388827929,
            "sparsity": 0.090376955134224,
            "magnitude": 0.6092892969988049,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.2650620511359527,
            "head_specialization": 0.6889540645681098,
            "reasoning_score": 0.3431186599812689
          },
          "16": {
            "entropy": 3.049104941450108,
            "head_specialization": 0.7173207703075831,
            "reasoning_score": 0.7120285712399649
          },
          "24": {
            "entropy": 0.7018730140825458,
            "head_specialization": 0.5702182846283388,
            "reasoning_score": 0.4996933264977353
          },
          "31": {
            "entropy": 2.513220670728841,
            "head_specialization": 0.8125254152889504,
            "reasoning_score": 0.29236203455198856
          }
        }
      },
      {
        "sample": 6,
        "question": "Which two body systems are directly involved in movement?",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.016981898138351034,
            "activation_std": 0.09864695001342766,
            "sparsity": 0.01127227730856867,
            "magnitude": 0.29564310186751735,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.05165936772319878,
            "activation_std": 0.022926376963733196,
            "sparsity": 0.0937070130122387,
            "magnitude": 1.1516846606005402,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.24109144279302216,
            "activation_std": 0.06758362342217197,
            "sparsity": 0.2176585164427257,
            "magnitude": 2.244334913361566,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.10736845553826922,
            "activation_std": 0.03041789148383067,
            "sparsity": 0.5252619211851451,
            "magnitude": 0.014098460253294923,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.0553702106223421,
            "activation_std": 0.0734352432029777,
            "sparsity": 0.306454623266592,
            "magnitude": 6.1683794352316,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.5771073191421872,
            "head_specialization": 0.5560937592337026,
            "reasoning_score": 0.806850384334454
          },
          "16": {
            "entropy": 1.6466492614747588,
            "head_specialization": 0.7262699943679737,
            "reasoning_score": 0.7082828552046169
          },
          "24": {
            "entropy": 0.7537814947379122,
            "head_specialization": 0.6342297414963893,
            "reasoning_score": 0.9092711280029695
          },
          "31": {
            "entropy": 0.9492636054885298,
            "head_specialization": 0.9437705564986597,
            "reasoning_score": 0.29373759740953137
          }
        }
      },
      {
        "sample": 7,
        "question": "Which change in the state of water particles causes the particles to become arranged in a fixed posi...",
        "expected": "C",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.026028687564917902,
            "activation_std": 0.04882442073595841,
            "sparsity": 0.22712894180300172,
            "magnitude": 2.5465076066213603,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.014165632617163136,
            "activation_std": 0.09941672237106602,
            "sparsity": 0.2761327226693564,
            "magnitude": 0.9270837546465397,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.058196518541649436,
            "activation_std": 0.04421369456893687,
            "sparsity": 0.3194547802649109,
            "magnitude": 3.4777309825953053,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.10014602145092931,
            "activation_std": 0.05027903553425214,
            "sparsity": 0.18158222445487782,
            "magnitude": 2.906526142070923,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.10451961522413766,
            "activation_std": 0.02799183708147326,
            "sparsity": 0.021966117475809156,
            "magnitude": 1.5264280416274376,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.613871167637679,
            "head_specialization": 0.6140636769187434,
            "reasoning_score": 0.4437920159902134
          },
          "16": {
            "entropy": 0.6675619055409638,
            "head_specialization": 0.5918325427866175,
            "reasoning_score": 0.5208399172401436
          },
          "24": {
            "entropy": 0.7153329231082315,
            "head_specialization": 0.7822397464317189,
            "reasoning_score": 0.7695947448745828
          },
          "31": {
            "entropy": 2.5285289800107345,
            "head_specialization": 0.75955618523318,
            "reasoning_score": 0.9140244950664592
          }
        }
      },
      {
        "sample": 8,
        "question": "Earth's core is primarily composed of which of the following materials?",
        "expected": "B",
        "generated": "B is correct",
        "correct": true,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.0535924945721928,
            "activation_std": 0.09065990138021651,
            "sparsity": 0.10120890703000829,
            "magnitude": 1.3147040593721973,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.04701270994698009,
            "activation_std": 0.05062508311205506,
            "sparsity": 0.05675540783311092,
            "magnitude": 1.5102453041758577,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.03382384247274244,
            "activation_std": 0.04298956823022196,
            "sparsity": 0.10183994864522573,
            "magnitude": 2.7305676704026416,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.09656261454099281,
            "activation_std": 0.026123742246470644,
            "sparsity": 0.0674594344945572,
            "magnitude": 0.701486251620156,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.017901732342353423,
            "activation_std": 0.05359643217681353,
            "sparsity": 0.14069653544536678,
            "magnitude": 0.992395963171683,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.6323895772315207,
            "head_specialization": 0.8234012383357151,
            "reasoning_score": 0.8313994490460068
          },
          "16": {
            "entropy": 1.3722311442320214,
            "head_specialization": 0.8845773883998124,
            "reasoning_score": 0.35568013459518033
          },
          "24": {
            "entropy": 0.9246288686451988,
            "head_specialization": 0.7038901219300225,
            "reasoning_score": 0.6510165108219915
          },
          "31": {
            "entropy": 0.9757038241637841,
            "head_specialization": 0.5612949692409316,
            "reasoning_score": 0.5469747748243303
          }
        }
      },
      {
        "sample": 9,
        "question": "Which of the following events during meiosis contributes most to the variation within a species?",
        "expected": "C",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.05225953962879576,
            "activation_std": 0.02437351400202166,
            "sparsity": 0.15394245843225363,
            "magnitude": 1.0734476774284114,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.06210215456682534,
            "activation_std": 0.14361494021997886,
            "sparsity": 0.09959993220855085,
            "magnitude": 0.733682818273242,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.10023508821826883,
            "activation_std": 0.0348687023011048,
            "sparsity": 0.15299220141666448,
            "magnitude": 0.5863653122229523,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.03509559448679352,
            "activation_std": 0.0895612746292119,
            "sparsity": 0.2672729304371749,
            "magnitude": 1.8234682665955928,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.1421372483408914,
            "activation_std": 0.12481316040309146,
            "sparsity": 0.1418554448044413,
            "magnitude": 0.7668749079493464,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.9509136147277114,
            "head_specialization": 0.905981437469645,
            "reasoning_score": 0.9496548540136133
          },
          "16": {
            "entropy": 3.5058395427274225,
            "head_specialization": 0.7286699973136765,
            "reasoning_score": 0.9288087635112754
          },
          "24": {
            "entropy": 0.728428496941826,
            "head_specialization": 0.9285253475807866,
            "reasoning_score": 0.9342984314123475
          },
          "31": {
            "entropy": 2.31623655027333,
            "head_specialization": 0.9109357098328922,
            "reasoning_score": 0.6248021853564591
          }
        }
      },
      {
        "sample": 10,
        "question": "Which of the following was probably most important in the formation of dark, fertile soil that is go...",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.024195360751534403,
            "activation_std": 0.027491781353473594,
            "sparsity": 0.05710050257137567,
            "magnitude": 0.22667355546978507,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.18065840221616958,
            "activation_std": 0.060497791099474056,
            "sparsity": 0.23311757523023655,
            "magnitude": 2.397720545698675,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.005065455178247296,
            "activation_std": 0.10479751828831133,
            "sparsity": 0.3678379293395291,
            "magnitude": 2.233882509984883,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.00969123978364525,
            "activation_std": 0.08611392645886845,
            "sparsity": 0.07087465509345305,
            "magnitude": 2.5246784117273613,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.11406658390555387,
            "activation_std": 0.05346065837710402,
            "sparsity": 0.19655073407996518,
            "magnitude": 1.1041051232529002,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.4220579013676655,
            "head_specialization": 0.45284924024141215,
            "reasoning_score": 0.5213222420941962
          },
          "16": {
            "entropy": 0.9597722976904575,
            "head_specialization": 0.9570010179324813,
            "reasoning_score": 0.7349538635356389
          },
          "24": {
            "entropy": 1.5752272013856552,
            "head_specialization": 0.7993396938528364,
            "reasoning_score": 0.9370687201860443
          },
          "31": {
            "entropy": 1.8558704835270863,
            "head_specialization": 0.9339502753677301,
            "reasoning_score": 0.754115867638307
          }
        }
      },
      {
        "sample": 11,
        "question": "When igneous rock is changed into metamorphic rock, which form of energy is this process?",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.0020460313390678287,
            "activation_std": 0.1490111092728998,
            "sparsity": 0.0559707572440331,
            "magnitude": 2.3897463305441735,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.14709264397332664,
            "activation_std": 0.04788946389576865,
            "sparsity": 0.22373720053618976,
            "magnitude": 1.6196773587648732,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.0081749853226419,
            "activation_std": 0.1784644652569115,
            "sparsity": 0.1747933214081483,
            "magnitude": 0.5656693337050488,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.017583238912829428,
            "activation_std": 0.0974859184068183,
            "sparsity": 0.22775822071745594,
            "magnitude": 0.9061767899670775,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.06166606175966832,
            "activation_std": 0.030195151928162408,
            "sparsity": 0.33485274180422975,
            "magnitude": 0.2472299921151874,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.6794859213772542,
            "head_specialization": 0.6921037788993635,
            "reasoning_score": 0.5715180765033162
          },
          "16": {
            "entropy": 2.0212525130387884,
            "head_specialization": 0.7168673177599764,
            "reasoning_score": 0.47070573717394526
          },
          "24": {
            "entropy": 1.2949352905018932,
            "head_specialization": 0.6731337382239737,
            "reasoning_score": 0.7850145818817401
          },
          "31": {
            "entropy": 1.9845883371548112,
            "head_specialization": 0.32133079212576804,
            "reasoning_score": 0.7955963218544202
          }
        }
      },
      {
        "sample": 12,
        "question": "In humans, the digestion process begins in",
        "expected": "B",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.04819066112677804,
            "activation_std": 0.08518500775814733,
            "sparsity": 0.03177741508060005,
            "magnitude": 2.399623187305398,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.09181496921265163,
            "activation_std": 0.0837109293138259,
            "sparsity": 0.13555360890098664,
            "magnitude": 9.845470785791543,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.005616893547769796,
            "activation_std": 0.059723107127983004,
            "sparsity": 0.5608512837911606,
            "magnitude": 0.09358398999359357,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.027399092060127146,
            "activation_std": 0.049497315744311154,
            "sparsity": 0.13599450826679915,
            "magnitude": 3.659003946890608,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.07754213075955897,
            "activation_std": 0.22362983810335366,
            "sparsity": 0.02625079425003,
            "magnitude": 5.103913953478056,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.671781572488531,
            "head_specialization": 0.7286793724090885,
            "reasoning_score": 0.7333081770354122
          },
          "16": {
            "entropy": 2.284366031685577,
            "head_specialization": 0.7656190867242844,
            "reasoning_score": 0.7495891252112394
          },
          "24": {
            "entropy": 1.0897711549573537,
            "head_specialization": 0.9282182220429048,
            "reasoning_score": 0.6034530471231943
          },
          "31": {
            "entropy": 0.8343324948900057,
            "head_specialization": 0.9617856268069535,
            "reasoning_score": 0.4126355375621359
          }
        }
      },
      {
        "sample": 13,
        "question": "Which of these items contains only a solution?",
        "expected": "B",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.03826157564249008,
            "activation_std": 0.10577481970415965,
            "sparsity": 0.5077155854065742,
            "magnitude": 0.054121496918430906,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.018555728167671422,
            "activation_std": 0.07836465379829068,
            "sparsity": 0.13605608326072727,
            "magnitude": 1.3837571436587452,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.01628938202057261,
            "activation_std": 0.02815391692081335,
            "sparsity": 0.11978027598711144,
            "magnitude": 3.054943603848213,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.035079830517957857,
            "activation_std": 0.05747278043791681,
            "sparsity": 0.12742038641117773,
            "magnitude": 1.6708337990285205,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.007421767993501054,
            "activation_std": 0.09134950422587451,
            "sparsity": 0.19240631503335265,
            "magnitude": 0.13383658455069852,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.3998874529171572,
            "head_specialization": 0.7870676733761255,
            "reasoning_score": 0.773126636747498
          },
          "16": {
            "entropy": 0.897572707985941,
            "head_specialization": 0.8587632526629162,
            "reasoning_score": 0.9106699611535397
          },
          "24": {
            "entropy": 1.7604218506216829,
            "head_specialization": 0.5222153767140518,
            "reasoning_score": 0.7678909662693851
          },
          "31": {
            "entropy": 1.8885484937959176,
            "head_specialization": 0.5418002790565593,
            "reasoning_score": 0.7130653600677045
          }
        }
      },
      {
        "sample": 14,
        "question": "Many natural rock formations change color over time. In Utah, for example, iron oxidized and formed ...",
        "expected": "A",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.16343650273678673,
            "activation_std": 0.09551706172692878,
            "sparsity": 0.29674853851917704,
            "magnitude": 5.562844557423379,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.05916564638376464,
            "activation_std": 0.07769092913620597,
            "sparsity": 0.31699690163220395,
            "magnitude": 0.12667909810945988,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.03718223638320554,
            "activation_std": 0.012823121063048448,
            "sparsity": 0.24882924065710763,
            "magnitude": 0.1098381553920758,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.20482284396289435,
            "activation_std": 0.17204800678333226,
            "sparsity": 0.2863230683831824,
            "magnitude": 0.9599996838040774,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.0781846625738321,
            "activation_std": 0.18460746836919267,
            "sparsity": 0.31338896636777913,
            "magnitude": 0.5185136975176546,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.4241358764412304,
            "head_specialization": 0.7672061271074482,
            "reasoning_score": 0.6171786845325822
          },
          "16": {
            "entropy": 0.5223627486343039,
            "head_specialization": 0.8416565334580808,
            "reasoning_score": 0.5281692067239748
          },
          "24": {
            "entropy": 1.6147784506468126,
            "head_specialization": 0.592440530294642,
            "reasoning_score": 0.7491488066911072
          },
          "31": {
            "entropy": 0.48750592464390097,
            "head_specialization": 0.8115319042699771,
            "reasoning_score": 0.7654643418810295
          }
        }
      },
      {
        "sample": 15,
        "question": "A population of small, plant-eating beetles lives in a forest. About half of the beetles are light b...",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.11507543997490893,
            "activation_std": 0.25050239277020764,
            "sparsity": 0.2733696009349983,
            "magnitude": 2.1462407502574754,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.0742331733898869,
            "activation_std": 0.014936220282222216,
            "sparsity": 0.10871517285326482,
            "magnitude": 0.6272020173842563,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.12321620335562873,
            "activation_std": 0.021417776304966808,
            "sparsity": 0.1198020480625251,
            "magnitude": 0.02939841143395017,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.0354878072495657,
            "activation_std": 0.04511294186760334,
            "sparsity": 0.08792145462490818,
            "magnitude": 0.20528371186084193,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.041590314059193445,
            "activation_std": 0.014283470289311538,
            "sparsity": 0.03262068640629027,
            "magnitude": 0.8069449365321234,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.2948761421892467,
            "head_specialization": 0.7539857307522064,
            "reasoning_score": 0.7570278580311212
          },
          "16": {
            "entropy": 0.8665779058634162,
            "head_specialization": 0.5845435104437504,
            "reasoning_score": 0.49846372786305015
          },
          "24": {
            "entropy": 0.7958245074900605,
            "head_specialization": 0.6035631857953327,
            "reasoning_score": 0.8693821682687842
          },
          "31": {
            "entropy": 0.5590263108038653,
            "head_specialization": 0.8296821574667093,
            "reasoning_score": 0.9304240545540229
          }
        }
      },
      {
        "sample": 16,
        "question": "A scientist wanting to document a change in a river's flow pattern should observe a river over a per...",
        "expected": "D",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.06760244181570747,
            "activation_std": 0.043426102261156535,
            "sparsity": 0.1205125098397655,
            "magnitude": 1.4789680752034708,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.04129408522957448,
            "activation_std": 0.09710892376930708,
            "sparsity": 0.09639715007350699,
            "magnitude": 0.08071574798533515,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.10781920040438822,
            "activation_std": 0.05764925614845771,
            "sparsity": 0.030221021629813577,
            "magnitude": 0.3283438076912432,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.14323787774089305,
            "activation_std": 0.1844720174353789,
            "sparsity": 0.2815870411555264,
            "magnitude": 6.209335213688647,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.01023097833299138,
            "activation_std": 0.09318171388190577,
            "sparsity": 0.6819534375923411,
            "magnitude": 1.021497357078763,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.8230299729739341,
            "head_specialization": 0.5968409612496077,
            "reasoning_score": 0.7930696947273815
          },
          "16": {
            "entropy": 2.420351078774036,
            "head_specialization": 0.8260972191468643,
            "reasoning_score": 0.3865052613650829
          },
          "24": {
            "entropy": 1.4748634780199836,
            "head_specialization": 0.799970694333395,
            "reasoning_score": 0.3289935874104797
          },
          "31": {
            "entropy": 0.8751159587063264,
            "head_specialization": 0.6038299785849149,
            "reasoning_score": 0.7553626412911983
          }
        }
      },
      {
        "sample": 17,
        "question": "Automobile engines built today are designed to be gas efficient. Gas-efficient engines most likely a...",
        "expected": "A",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.04901344700033954,
            "activation_std": 0.09602566982712885,
            "sparsity": 0.2883758396133579,
            "magnitude": 0.592555775177597,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.04819660144236658,
            "activation_std": 0.1699238211848585,
            "sparsity": 0.17259346673660453,
            "magnitude": 0.5517336819266081,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.09209228943685949,
            "activation_std": 0.040078653968843275,
            "sparsity": 0.3498243094438571,
            "magnitude": 1.1476096130256783,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.04876276481313038,
            "activation_std": 0.08820007518290368,
            "sparsity": 0.6393825532367244,
            "magnitude": 1.271103766713173,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.09982618658051673,
            "activation_std": 0.16717488044982887,
            "sparsity": 0.33561976115728503,
            "magnitude": 2.2393959240119523,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.6944182578189688,
            "head_specialization": 0.713945287760714,
            "reasoning_score": 0.5640234914830857
          },
          "16": {
            "entropy": 0.8411154638078929,
            "head_specialization": 0.6994450345837164,
            "reasoning_score": 0.7335996797621539
          },
          "24": {
            "entropy": 2.100099436710056,
            "head_specialization": 0.5328960605044242,
            "reasoning_score": 0.666909915305688
          },
          "31": {
            "entropy": 1.6593048248236777,
            "head_specialization": 0.8771876375749085,
            "reasoning_score": 0.6294924698858028
          }
        }
      },
      {
        "sample": 18,
        "question": "A student in an empty classroom shouts, \"Hello!\" Which best explains what the student hears after th...",
        "expected": "B",
        "generated": "The answer is A",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.22319869403241782,
            "activation_std": 0.27377325362705535,
            "sparsity": 0.11899072904944841,
            "magnitude": 0.3960402718253371,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.18133376123113096,
            "activation_std": 0.10900235892355592,
            "sparsity": 0.2422830697829957,
            "magnitude": 1.9907652832721459,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.1893951480349081,
            "activation_std": 0.029917245402318094,
            "sparsity": 0.22982856334260698,
            "magnitude": 0.36249920564584037,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.007138660230547662,
            "activation_std": 0.05151141382486523,
            "sparsity": 0.26602623196713016,
            "magnitude": 5.814471393254394,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.08291325668192884,
            "activation_std": 0.09808801653212348,
            "sparsity": 0.1735808272768908,
            "magnitude": 0.6709649803869872,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.1403755222437506,
            "head_specialization": 0.7166914853624575,
            "reasoning_score": 0.9467570994092948
          },
          "16": {
            "entropy": 0.9477107188582523,
            "head_specialization": 0.9646450195692741,
            "reasoning_score": 0.7034800219351026
          },
          "24": {
            "entropy": 0.7522151563429087,
            "head_specialization": 0.7344465750057041,
            "reasoning_score": 0.17951933250652305
          },
          "31": {
            "entropy": 0.7906811956306761,
            "head_specialization": 0.5776335241575531,
            "reasoning_score": 0.6818616811563675
          }
        }
      },
      {
        "sample": 19,
        "question": "Which type of energy in gasoline is transformed into mechanical energy in a motorcycle engine?",
        "expected": "1",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.15585480258533385,
            "activation_std": 0.189839899393222,
            "sparsity": 0.152711671030213,
            "magnitude": 0.7872451437520047,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.057960713207444406,
            "activation_std": 0.22424319891045552,
            "sparsity": 0.04630632478709914,
            "magnitude": 1.2511409459391016,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.15504659422223632,
            "activation_std": 0.13615976704270638,
            "sparsity": 0.15489371171996494,
            "magnitude": 1.1303426428046424,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.06565181565177629,
            "activation_std": 0.2534412068017596,
            "sparsity": 0.06729615808970743,
            "magnitude": 7.53031671269707,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.057431922904721024,
            "activation_std": 0.02229647347120217,
            "sparsity": 0.11275057922957064,
            "magnitude": 1.5543448413018894,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.946904802106911,
            "head_specialization": 0.5080503367784424,
            "reasoning_score": 0.5766916557103096
          },
          "16": {
            "entropy": 0.37701896556876635,
            "head_specialization": 0.8978766723178682,
            "reasoning_score": 0.46140137216126675
          },
          "24": {
            "entropy": 1.0272208358197819,
            "head_specialization": 0.7625634836050225,
            "reasoning_score": 0.27083103261428376
          },
          "31": {
            "entropy": 0.961059653129765,
            "head_specialization": 0.8532590202428622,
            "reasoning_score": 0.7774671322454624
          }
        }
      }
    ]
  },
  "SVAMP_4bit": {
    "dataset": "SVAMP",
    "bits": 4,
    "accuracy": 0.05,
    "correct": 1,
    "total": 20,
    "time": 0.00109100341796875,
    "device": "cpu",
    "critical_layers_affected": [],
    "layer_analysis_summary": {
      "0": {
        "sparsity": 0.13702728480325008,
        "critical": false
      },
      "8": {
        "sparsity": 0.3035038987076288,
        "critical": false
      },
      "16": {
        "sparsity": 0.013428066287066344,
        "critical": true
      },
      "24": {
        "sparsity": 0.10930912848728608,
        "critical": true
      },
      "31": {
        "sparsity": 0.19185949053981607,
        "critical": true
      }
    },
    "attention_summary": {
      "8": {
        "entropy": 0.6289451807252622,
        "head_specialization": 0.7855559361512803,
        "reasoning_score": 0.6114662997590598
      },
      "16": {
        "entropy": 2.38683670535191,
        "head_specialization": 0.38136504423167633,
        "reasoning_score": 0.6475459322178869
      },
      "24": {
        "entropy": 1.5631994858858809,
        "head_specialization": 0.8419963796205152,
        "reasoning_score": 0.9072280863205407
      },
      "31": {
        "entropy": 2.374786441445595,
        "head_specialization": 0.818361684771658,
        "reasoning_score": 0.95139813217265
      }
    },
    "detailed_results": [
      {
        "sample": 0,
        "question": "There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 gro...",
        "expected": "145",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.12313538032062575,
            "activation_std": 0.2770863899722796,
            "sparsity": 0.06666236222390688,
            "magnitude": 1.1132442749412554,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.03730555030708511,
            "activation_std": 0.06611606159258189,
            "sparsity": 0.09855397927155046,
            "magnitude": 1.733981528103492,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.06919944482844681,
            "activation_std": 0.038025187763427726,
            "sparsity": 0.04379475843656935,
            "magnitude": 1.7161155152110976,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.0037888284948500318,
            "activation_std": 0.07029249897204902,
            "sparsity": 0.341591007479767,
            "magnitude": 1.6817416717584965,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.013102283518424976,
            "activation_std": 0.059775884777624244,
            "sparsity": 0.3062982344416382,
            "magnitude": 0.4908857515536883,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.340692088169132,
            "head_specialization": 0.5773894484026774,
            "reasoning_score": 0.8362679077480915
          },
          "16": {
            "entropy": 0.4158787358056343,
            "head_specialization": 0.7141691956120805,
            "reasoning_score": 0.74439859710337
          },
          "24": {
            "entropy": 0.7855121909373564,
            "head_specialization": 0.5433778612093971,
            "reasoning_score": 0.6125459676500933
          },
          "31": {
            "entropy": 0.8764337004613388,
            "head_specialization": 0.9499450400476321,
            "reasoning_score": 0.4935229214874341
          }
        }
      },
      {
        "sample": 1,
        "question": "Marco and his dad went strawberry picking. Marco's dad's strawberries weighed 11 pounds. If together...",
        "expected": "19",
        "generated": "B is correct",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.10783176061439888,
            "activation_std": 0.06683072551682401,
            "sparsity": 0.13616071536343666,
            "magnitude": 0.5941801263104095,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.06609800711316666,
            "activation_std": 0.24017913402804947,
            "sparsity": 0.17992180568367197,
            "magnitude": 1.0497084993603445,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.08860336877177641,
            "activation_std": 0.10161008255429832,
            "sparsity": 0.2380146169757218,
            "magnitude": 2.0229876480455182,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.012403191387087715,
            "activation_std": 0.05773580712402752,
            "sparsity": 0.11132297437608907,
            "magnitude": 2.8949308276951324,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.11666158400334192,
            "activation_std": 0.045978892961321724,
            "sparsity": 0.25465167393513194,
            "magnitude": 1.204817758814837,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.518078486193549,
            "head_specialization": 0.6324182490704442,
            "reasoning_score": 0.872238706893251
          },
          "16": {
            "entropy": 1.1118059196640373,
            "head_specialization": 0.527305388368202,
            "reasoning_score": 0.9021815116699902
          },
          "24": {
            "entropy": 1.5356242200454568,
            "head_specialization": 0.8988081118885313,
            "reasoning_score": 0.5133095429497239
          },
          "31": {
            "entropy": 2.061893885348392,
            "head_specialization": 0.8547411216525383,
            "reasoning_score": 0.6873344911780931
          }
        }
      },
      {
        "sample": 2,
        "question": "Edward spent $ 6 to buy 2 books each book costing him the same amount of money. Now he has $ 12. How...",
        "expected": "3",
        "generated": "3",
        "correct": true,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.09839051310578356,
            "activation_std": 0.02413779433541062,
            "sparsity": 0.2394306990327693,
            "magnitude": 1.8233621639678916,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.04494153600085971,
            "activation_std": 0.056693525925913216,
            "sparsity": 0.15784078927333314,
            "magnitude": 2.457815835223802,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.04051788771987651,
            "activation_std": 0.05588474968204804,
            "sparsity": 0.35017433389305397,
            "magnitude": 0.6231192821436186,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.1271695109905789,
            "activation_std": 0.11785049211038189,
            "sparsity": 0.4384834330784133,
            "magnitude": 2.4921110961170716,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.27631627538400694,
            "activation_std": 0.06627290928848215,
            "sparsity": 0.12156135977698068,
            "magnitude": 0.9223572977512736,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.5075068524955073,
            "head_specialization": 0.5566465122766443,
            "reasoning_score": 0.6315774658218126
          },
          "16": {
            "entropy": 3.0629773413433203,
            "head_specialization": 0.9329809407512775,
            "reasoning_score": 0.5915820806252428
          },
          "24": {
            "entropy": 0.9771526203092799,
            "head_specialization": 0.6042672384491107,
            "reasoning_score": 0.808864422127571
          },
          "31": {
            "entropy": 1.6679552055833593,
            "head_specialization": 0.3172464434530458,
            "reasoning_score": 0.15268028808450512
          }
        }
      },
      {
        "sample": 3,
        "question": "Frank was reading through his favorite book. The book had 3 chapters, each with the same number of p...",
        "expected": "198",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.016079556116126208,
            "activation_std": 0.19502933392498478,
            "sparsity": 0.1419765771154366,
            "magnitude": 0.20169113447199372,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.09964067737417698,
            "activation_std": 0.08109777606522361,
            "sparsity": 0.1578733511714131,
            "magnitude": 2.518472526178466,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.054809181623198505,
            "activation_std": 0.218731176341047,
            "sparsity": 0.06021033924469929,
            "magnitude": 3.680492676576161,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.02569172556315213,
            "activation_std": 0.2361832542964272,
            "sparsity": 0.09392659771190899,
            "magnitude": 1.9106444117615504,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.17375257341577263,
            "activation_std": 0.13188496818386206,
            "sparsity": 0.09160342760049713,
            "magnitude": 0.6017923392163358,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.232763565074673,
            "head_specialization": 0.4748361673961962,
            "reasoning_score": 0.8892054209051835
          },
          "16": {
            "entropy": 2.4443101689092614,
            "head_specialization": 0.9003009771899632,
            "reasoning_score": 0.800631982845566
          },
          "24": {
            "entropy": 1.0155093747072734,
            "head_specialization": 0.46965895384003337,
            "reasoning_score": 0.7889371829430959
          },
          "31": {
            "entropy": 1.933579220852651,
            "head_specialization": 0.5895935395105709,
            "reasoning_score": 0.6366856327812942
          }
        }
      },
      {
        "sample": 4,
        "question": "There were 78 dollars in Olivia's wallet. She spent 15 dollars at a supermarket. How much money does...",
        "expected": "63",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.08066130802203807,
            "activation_std": 0.0205084052028267,
            "sparsity": 0.28254138751213703,
            "magnitude": 0.3958084191901564,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.10092019757724731,
            "activation_std": 0.008417324125461647,
            "sparsity": 0.15651987008689222,
            "magnitude": 5.3610168133241025,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.011708128559849491,
            "activation_std": 0.06521147483857014,
            "sparsity": 0.08523354406590967,
            "magnitude": 0.7697149154527072,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.09834898489599775,
            "activation_std": 0.06800710051570866,
            "sparsity": 0.13489510742539734,
            "magnitude": 0.16078232401283854,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.01652548818388638,
            "activation_std": 0.035836570213479906,
            "sparsity": 0.32552268783561583,
            "magnitude": 5.716475224681955,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.962002278550548,
            "head_specialization": 0.8050913000492429,
            "reasoning_score": 0.8380883184169636
          },
          "16": {
            "entropy": 2.8420579789153266,
            "head_specialization": 0.9198121430170364,
            "reasoning_score": 0.6913540981052914
          },
          "24": {
            "entropy": 0.8156437485560795,
            "head_specialization": 0.9194065758241199,
            "reasoning_score": 0.9789407659581499
          },
          "31": {
            "entropy": 0.2135315396359716,
            "head_specialization": 0.909136458880906,
            "reasoning_score": 0.1926301516054747
          }
        }
      },
      {
        "sample": 5,
        "question": "Paul got a box of 110 crayons for his birthday. During the school year he gave 90 crayons to his fri...",
        "expected": "322",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.015931910525054327,
            "activation_std": 0.05647074423764788,
            "sparsity": 0.20323867704697593,
            "magnitude": 1.27171723923352,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.010649950913322633,
            "activation_std": 0.117758774528614,
            "sparsity": 0.027788636769951722,
            "magnitude": 0.06193791792473193,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.03946355084110496,
            "activation_std": 0.045306723162588824,
            "sparsity": 0.38071758261953986,
            "magnitude": 0.6389761027713391,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.13835918664772875,
            "activation_std": 0.021219612989531178,
            "sparsity": 0.4366134119422588,
            "magnitude": 2.0464271845545037,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.08435609233199608,
            "activation_std": 0.041779640223895964,
            "sparsity": 0.14096509460822765,
            "magnitude": 0.15779449916569893,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.2808316614179143,
            "head_specialization": 0.7633090374728269,
            "reasoning_score": 0.6992336360622643
          },
          "16": {
            "entropy": 1.2674977052184286,
            "head_specialization": 0.6970208026021861,
            "reasoning_score": 0.4597889892834518
          },
          "24": {
            "entropy": 2.1741578266578347,
            "head_specialization": 0.5910487070311934,
            "reasoning_score": 0.6806116533172945
          },
          "31": {
            "entropy": 1.7756498158692926,
            "head_specialization": 0.6397441642132434,
            "reasoning_score": 0.7007698621705327
          }
        }
      },
      {
        "sample": 6,
        "question": "Randy has 95 blocks. He uses 20 blocks to build a house and 50 blocks to build a tower. How many mor...",
        "expected": "30",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.04945197233209633,
            "activation_std": 0.2097890116207519,
            "sparsity": 0.1822556118031219,
            "magnitude": 5.721149531248923,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.07788924743274317,
            "activation_std": 0.0765420665239326,
            "sparsity": 0.3756222015895844,
            "magnitude": 2.64797904286129,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.03177553139005084,
            "activation_std": 0.07790222949986422,
            "sparsity": 0.017045610843715923,
            "magnitude": 0.7152255323155489,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.057350773875211425,
            "activation_std": 0.16348684738561778,
            "sparsity": 0.363940894062787,
            "magnitude": 0.015849029298948252,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.07034486687918322,
            "activation_std": 0.10106464339811726,
            "sparsity": 0.3547520993945966,
            "magnitude": 0.5813492535383302,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.41947805545202704,
            "head_specialization": 0.47572422215939664,
            "reasoning_score": 0.7066008567557612
          },
          "16": {
            "entropy": 0.55265706363576,
            "head_specialization": 0.4917780699694342,
            "reasoning_score": 0.44992730961989097
          },
          "24": {
            "entropy": 1.199461495187099,
            "head_specialization": 0.7495088868589418,
            "reasoning_score": 0.47737330196005484
          },
          "31": {
            "entropy": 0.17129671855665263,
            "head_specialization": 0.7729665796788723,
            "reasoning_score": 0.6494402408951054
          }
        }
      },
      {
        "sample": 7,
        "question": "After Jessie started to go jogging everyday she lost 126 kilograms. She currently weighs 66 kilogram...",
        "expected": "192",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.2109774752797527,
            "activation_std": 0.10710133399019456,
            "sparsity": 0.2137350839561622,
            "magnitude": 0.3921031944749025,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.10076931860144703,
            "activation_std": 0.10029798667292794,
            "sparsity": 0.179380187047473,
            "magnitude": 0.9769196259074546,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.09484245872221857,
            "activation_std": 0.01725058940209045,
            "sparsity": 0.02600146478607137,
            "magnitude": 0.27315857275018435,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.1286792839987892,
            "activation_std": 0.062325754784811595,
            "sparsity": 0.32314948171902474,
            "magnitude": 2.3867981549918875,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.020472644677447555,
            "activation_std": 0.05598364216659223,
            "sparsity": 0.22498586901392595,
            "magnitude": 5.288774313776572,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.046393769793529,
            "head_specialization": 0.8247672965001527,
            "reasoning_score": 0.5201004397922111
          },
          "16": {
            "entropy": 4.055797351420344,
            "head_specialization": 0.8392916074738764,
            "reasoning_score": 0.5201517849746808
          },
          "24": {
            "entropy": 0.8568047076054685,
            "head_specialization": 0.6445916838939644,
            "reasoning_score": 0.3454955152919778
          },
          "31": {
            "entropy": 1.1238599932998201,
            "head_specialization": 0.9843211590022665,
            "reasoning_score": 0.76041425240715
          }
        }
      },
      {
        "sample": 8,
        "question": "At the arcade Dave had won some tickets. He used 12 tickets to buy some toys. If he still has 14 tic...",
        "expected": "26",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.04767789843863676,
            "activation_std": 0.04212086534068854,
            "sparsity": 0.39473270354238127,
            "magnitude": 0.8839103334566466,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.004833260240973814,
            "activation_std": 0.05161281847741701,
            "sparsity": 0.2173802677291573,
            "magnitude": 1.9502571677652059,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.11519872498240114,
            "activation_std": 0.08375293652934332,
            "sparsity": 0.25759492299265774,
            "magnitude": 1.7590834682395537,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.11790683373038789,
            "activation_std": 0.06148493256017926,
            "sparsity": 0.33068694357233575,
            "magnitude": 2.327847737179284,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.19308054149694479,
            "activation_std": 0.16558444148915252,
            "sparsity": 0.311720106542506,
            "magnitude": 2.9191267351150443,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.9174208234323336,
            "head_specialization": 0.9797352803732359,
            "reasoning_score": 0.8948510419052769
          },
          "16": {
            "entropy": 2.5442543333899543,
            "head_specialization": 0.7603718412037008,
            "reasoning_score": 0.5654030026857544
          },
          "24": {
            "entropy": 2.6566834330895723,
            "head_specialization": 0.41477977230650925,
            "reasoning_score": 0.1645845979386707
          },
          "31": {
            "entropy": 1.1786075519539814,
            "head_specialization": 0.5175336957162724,
            "reasoning_score": 0.6611231262155552
          }
        }
      },
      {
        "sample": 9,
        "question": "Bryan took a look at his books as well. If he has 34 books distributed equally in 2 bookshelves How ...",
        "expected": "17",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.08828667576757827,
            "activation_std": 0.027416586672827745,
            "sparsity": 0.11448037655834468,
            "magnitude": 1.6678263845273518,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.00015712886007108773,
            "activation_std": 0.06583185219864597,
            "sparsity": 0.26280283296611523,
            "magnitude": 1.2533879666000158,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.032441605232844686,
            "activation_std": 0.23872265075042334,
            "sparsity": 0.051404290130378616,
            "magnitude": 6.317227570426423,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.052016199584487166,
            "activation_std": 0.014317260008206257,
            "sparsity": 0.17567448686788517,
            "magnitude": 2.0293307006186376,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.16990585451204077,
            "activation_std": 0.03978855026305203,
            "sparsity": 0.20120011066034477,
            "magnitude": 3.17560402536676,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.31084138375119774,
            "head_specialization": 0.7192407859642203,
            "reasoning_score": 0.3185104398336211
          },
          "16": {
            "entropy": 2.5820896287184993,
            "head_specialization": 0.49526588125654436,
            "reasoning_score": 0.7529116959672024
          },
          "24": {
            "entropy": 1.8994466939362544,
            "head_specialization": 0.8136688388069914,
            "reasoning_score": 0.8660544938430912
          },
          "31": {
            "entropy": 0.43827358122405446,
            "head_specialization": 0.6283634015758559,
            "reasoning_score": 0.8081835516838617
          }
        }
      },
      {
        "sample": 10,
        "question": "Dave had 15 apps on his phone. He added 71 new apps. After deleting some he had 14 left. How many mo...",
        "expected": "1",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.045329584463827,
            "activation_std": 0.08907349828416561,
            "sparsity": 0.22259407031176676,
            "magnitude": 1.9738236134400355,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.13052610745624404,
            "activation_std": 0.059551695319322466,
            "sparsity": 0.027843280179507834,
            "magnitude": 0.05075054279617598,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.13375259487883992,
            "activation_std": 0.353620119626903,
            "sparsity": 0.07550753055858664,
            "magnitude": 3.0158090257770924,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.010726208856574132,
            "activation_std": 0.02851355897338714,
            "sparsity": 0.13676594672664558,
            "magnitude": 2.0799579430032056,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.05959460019518969,
            "activation_std": 0.036834204916283274,
            "sparsity": 0.22385313096748655,
            "magnitude": 0.8945055788904427,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.4480829688153714,
            "head_specialization": 0.8540333752182944,
            "reasoning_score": 0.22672884626978002
          },
          "16": {
            "entropy": 1.348437981707463,
            "head_specialization": 0.8096194304608422,
            "reasoning_score": 0.8251994310608884
          },
          "24": {
            "entropy": 0.9716381137634672,
            "head_specialization": 0.860295769074804,
            "reasoning_score": 0.7566072847926892
          },
          "31": {
            "entropy": 3.5780367039194987,
            "head_specialization": 0.35188471132811383,
            "reasoning_score": 0.8114788411696449
          }
        }
      },
      {
        "sample": 11,
        "question": "Adam has 4 more apples than Jackie. Together Adam and Jackie have 14 apples. Bob has 6 apples more t...",
        "expected": "20",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.05941119681218057,
            "activation_std": 0.13018828256282305,
            "sparsity": 0.065531550638943,
            "magnitude": 1.9843097845085846,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.04926570739258186,
            "activation_std": 0.13733894041662503,
            "sparsity": 0.06780740640730858,
            "magnitude": 2.303387030096746,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.019883054247604344,
            "activation_std": 0.15966526826158545,
            "sparsity": 0.24940420145721617,
            "magnitude": 0.2004526576274478,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.0727767274685962,
            "activation_std": 0.10002984296299262,
            "sparsity": 0.49707590291540393,
            "magnitude": 1.0937834664713324,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.04596336838730963,
            "activation_std": 0.17526077913530952,
            "sparsity": 0.09995468477829356,
            "magnitude": 1.3176047214520923,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.6454508769900377,
            "head_specialization": 0.6674753615167413,
            "reasoning_score": 0.7115946282892759
          },
          "16": {
            "entropy": 3.1154398424662366,
            "head_specialization": 0.5774023668892578,
            "reasoning_score": 0.34931316992975214
          },
          "24": {
            "entropy": 1.9490459235311168,
            "head_specialization": 0.7611117737244708,
            "reasoning_score": 0.5724116779228593
          },
          "31": {
            "entropy": 1.4426865465577345,
            "head_specialization": 0.4538856535153166,
            "reasoning_score": 0.6513674310315366
          }
        }
      },
      {
        "sample": 12,
        "question": "You have 104 dollars. How many packs of dvds can you buy if each pack costs 26 dollars?",
        "expected": "4",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.16861687844611833,
            "activation_std": 0.05076035650183172,
            "sparsity": 0.1350629645554105,
            "magnitude": 0.48382550196879354,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.04607095158759167,
            "activation_std": 0.11317151500815842,
            "sparsity": 0.05138250356342279,
            "magnitude": 0.24901399006228098,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.054104142355145574,
            "activation_std": 0.03764430429223101,
            "sparsity": 0.014808801987683496,
            "magnitude": 1.80165947242235,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.022846938073741337,
            "activation_std": 0.17575208905952683,
            "sparsity": 0.09610950859640338,
            "magnitude": 3.988877544072104,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.0484398999980253,
            "activation_std": 0.05466449670092141,
            "sparsity": 0.2567718781633324,
            "magnitude": 0.5425980658412303,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.5956031792235166,
            "head_specialization": 0.758003701194314,
            "reasoning_score": 0.8204685061257277
          },
          "16": {
            "entropy": 0.7619423145450825,
            "head_specialization": 0.8689022613675389,
            "reasoning_score": 0.9638266071435307
          },
          "24": {
            "entropy": 1.6564661595178176,
            "head_specialization": 0.9063555409831838,
            "reasoning_score": 0.6239927261909297
          },
          "31": {
            "entropy": 1.585147332954172,
            "head_specialization": 0.4830278599036959,
            "reasoning_score": 0.7837408342718485
          }
        }
      },
      {
        "sample": 13,
        "question": "Jerry had 7 books and 3 action figures on a shelf in his room. Later he added 2 more action figures ...",
        "expected": "2",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.02937474627895974,
            "activation_std": 0.12423980107835714,
            "sparsity": 0.03737702975508783,
            "magnitude": 1.6322949730640044,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.12345302545778863,
            "activation_std": 0.032535308725673825,
            "sparsity": 0.14380039323129257,
            "magnitude": 1.3523368457510627,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.03004686390459866,
            "activation_std": 0.22122343697564217,
            "sparsity": 0.2135002146575607,
            "magnitude": 1.4975108930681214,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.1442549626809078,
            "activation_std": 0.19698485871980947,
            "sparsity": 0.07830232851004156,
            "magnitude": 0.5140185726007968,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.06995968477102314,
            "activation_std": 0.08602357993368226,
            "sparsity": 0.17435587301609404,
            "magnitude": 1.2671903185366733,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.0121449098734532,
            "head_specialization": 0.6475561593942643,
            "reasoning_score": 0.9427492479466553
          },
          "16": {
            "entropy": 0.964754283881336,
            "head_specialization": 0.9248045633091085,
            "reasoning_score": 0.6332021782434449
          },
          "24": {
            "entropy": 1.0453051290583377,
            "head_specialization": 0.6901978597957297,
            "reasoning_score": 0.5021016478787443
          },
          "31": {
            "entropy": 1.2792643928860759,
            "head_specialization": 0.29969571566922093,
            "reasoning_score": 0.657250111469329
          }
        }
      },
      {
        "sample": 14,
        "question": "Melissa scored a total of 91 points in 13 games scoring the same for each game. How many points did ...",
        "expected": "7",
        "generated": "The answer is A",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.028701510456328468,
            "activation_std": 0.045198389952675486,
            "sparsity": 0.1655907425962219,
            "magnitude": 0.3972510464951863,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.12274042603151628,
            "activation_std": 0.06411593642906703,
            "sparsity": 0.24270723518806753,
            "magnitude": 0.24244527736004085,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.16187794190327376,
            "activation_std": 0.09541967565938475,
            "sparsity": 0.5698497540699607,
            "magnitude": 2.45491030157074,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.16326020265416416,
            "activation_std": 0.08182294232223217,
            "sparsity": 0.18582593991483995,
            "magnitude": 1.319341839479173,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.10652102911095414,
            "activation_std": 0.13002777300878948,
            "sparsity": 0.3400840847521861,
            "magnitude": 2.3676522868665746,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.347490093283133,
            "head_specialization": 0.5908348231381552,
            "reasoning_score": 0.7299775132759259
          },
          "16": {
            "entropy": 1.447482378447634,
            "head_specialization": 0.8153015297501945,
            "reasoning_score": 0.8612202105492321
          },
          "24": {
            "entropy": 0.28958292626253734,
            "head_specialization": 0.5517606912877328,
            "reasoning_score": 0.7683337723285802
          },
          "31": {
            "entropy": 1.1051086111728652,
            "head_specialization": 0.6197743799834274,
            "reasoning_score": 0.9545217640338477
          }
        }
      },
      {
        "sample": 15,
        "question": "Faye had 35 packs of pencils each one having 4 pencils. She was placing her pencils into rows with 2...",
        "expected": "70",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.022598235552373336,
            "activation_std": 0.06357537150804528,
            "sparsity": 0.19509696834063073,
            "magnitude": 0.7806078019554646,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.0014926023412002772,
            "activation_std": 0.03929369751398373,
            "sparsity": 0.29874301704409156,
            "magnitude": 1.8579191716467363,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.06082354114437077,
            "activation_std": 0.012461875478168037,
            "sparsity": 0.08900755423360171,
            "magnitude": 2.3166450486426604,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.06819578620277576,
            "activation_std": 0.07251191978397263,
            "sparsity": 0.26835113231449165,
            "magnitude": 1.9097949948995603,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.16373301568362142,
            "activation_std": 0.1817923027918727,
            "sparsity": 0.07800124248881386,
            "magnitude": 1.064873232488462,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.0866808901720204,
            "head_specialization": 0.8032539836658831,
            "reasoning_score": 0.6257771733425682
          },
          "16": {
            "entropy": 1.1119974161838997,
            "head_specialization": 0.9368179204821236,
            "reasoning_score": 0.5029639065500177
          },
          "24": {
            "entropy": 0.6234819572432411,
            "head_specialization": 0.7226098258814165,
            "reasoning_score": 0.9160419304732622
          },
          "31": {
            "entropy": 1.3718253025048608,
            "head_specialization": 0.6665486510446808,
            "reasoning_score": 0.7158520925199491
          }
        }
      },
      {
        "sample": 16,
        "question": "Each basket of peaches has 19 red peaches and 4 green peaches. If there are 15 such baskets How many...",
        "expected": "345",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.13228985680962305,
            "activation_std": 0.09196558064355231,
            "sparsity": 0.03737466284929283,
            "magnitude": 0.4044289288770914,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.0003043857084955339,
            "activation_std": 0.08129469588828749,
            "sparsity": 0.412508734019803,
            "magnitude": 2.6970796644617625,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.15760622877075586,
            "activation_std": 0.05254590892340843,
            "sparsity": 0.18392020035280038,
            "magnitude": 0.6654427030275976,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.17034499771930955,
            "activation_std": 0.09281384170420146,
            "sparsity": 0.14693988751470985,
            "magnitude": 1.929098186291974,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.12867542943701354,
            "activation_std": 0.10524371247864668,
            "sparsity": 0.10137738499998081,
            "magnitude": 1.6320928089567819,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.408858283499302,
            "head_specialization": 0.8684566494316553,
            "reasoning_score": 0.6642179059766448
          },
          "16": {
            "entropy": 0.6036106810924078,
            "head_specialization": 0.8417508905517822,
            "reasoning_score": 0.9516295737056796
          },
          "24": {
            "entropy": 0.8634445430893086,
            "head_specialization": 0.7482581804607822,
            "reasoning_score": 0.5852110928427618
          },
          "31": {
            "entropy": 0.7831333269605756,
            "head_specialization": 0.8325382584324036,
            "reasoning_score": 0.8951547425180397
          }
        }
      },
      {
        "sample": 17,
        "question": "Nell collects cards. She had 309 baseball cards and 356 Ace cards. She gave some of her cards to Jef...",
        "expected": "266",
        "generated": "The answer is A",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.0869876389144218,
            "activation_std": 0.015832878728709487,
            "sparsity": 0.22734264262024026,
            "magnitude": 1.3233666730591334,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.1693693841627153,
            "activation_std": 0.023468294252059076,
            "sparsity": 0.15084279761923394,
            "magnitude": 0.5459455841225738,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.03215584517114472,
            "activation_std": 0.16596238692114307,
            "sparsity": 0.26492361792700836,
            "magnitude": 0.29069858858018977,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.08733248464447566,
            "activation_std": 0.09160697031205292,
            "sparsity": 0.13377877640970615,
            "magnitude": 0.44569386167216796,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.03744619000083622,
            "activation_std": 0.09534938165517504,
            "sparsity": 0.3109526346078992,
            "magnitude": 1.7070738206590823,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 7.145205854808982,
            "head_specialization": 0.5808146653604508,
            "reasoning_score": 0.8356701993108936
          },
          "16": {
            "entropy": 2.891182405149391,
            "head_specialization": 0.32365688005992754,
            "reasoning_score": 0.8754780074060807
          },
          "24": {
            "entropy": 1.236912379850101,
            "head_specialization": 0.6178997765095781,
            "reasoning_score": 0.7632185758806866
          },
          "31": {
            "entropy": 0.7459882517351019,
            "head_specialization": 0.7617597915402156,
            "reasoning_score": 0.8457074240972956
          }
        }
      },
      {
        "sample": 18,
        "question": "There are 270 students in a school. If the school has 5 students in each grades and each grade has t...",
        "expected": "54",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.12363088053351555,
            "activation_std": 0.10885717191812393,
            "sparsity": 0.1230809323734759,
            "magnitude": 0.6344677881418017,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.0667709821673073,
            "activation_std": 0.17696579769180326,
            "sparsity": 0.10970569389486524,
            "magnitude": 0.49375228119546344,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.13246831171992077,
            "activation_std": 0.02522508962367539,
            "sparsity": 0.24729076919972615,
            "magnitude": 1.2183448945353421,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.23010067018803407,
            "activation_std": 0.11124289765866956,
            "sparsity": 0.36927152933916857,
            "magnitude": 1.04778913879473,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.0429088599039229,
            "activation_std": 0.2205978790485765,
            "sparsity": 0.10224687465077018,
            "magnitude": 1.9388428240059339,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.435540782677607,
            "head_specialization": 0.6641680825245028,
            "reasoning_score": 0.7909001639793007
          },
          "16": {
            "entropy": 1.4299968034116155,
            "head_specialization": 0.9864942419618398,
            "reasoning_score": 0.685602386776394
          },
          "24": {
            "entropy": 2.182721040133726,
            "head_specialization": 0.7229259699203593,
            "reasoning_score": 0.4792983106342608
          },
          "31": {
            "entropy": 0.7292732328362996,
            "head_specialization": 0.4858815836170447,
            "reasoning_score": 0.8436730699883561
          }
        }
      },
      {
        "sample": 19,
        "question": "In a school there are 34 girls and 841 boys. How many more boys than girls does the school have?",
        "expected": "807",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.07280291374885231,
            "activation_std": 0.14518786590265836,
            "sparsity": 0.13702728480325008,
            "magnitude": 0.19590667748750107,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.23221167459586087,
            "activation_std": 0.040417303937987065,
            "sparsity": 0.3035038987076288,
            "magnitude": 1.4356686759482369,
            "quantization_bits": 4,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.1079136261609076,
            "activation_std": 0.09914411731363765,
            "sparsity": 0.013428066287066344,
            "magnitude": 1.059253196811857,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.04343457936644633,
            "activation_std": 0.07716433572183518,
            "sparsity": 0.10930912848728608,
            "magnitude": 1.5364912959068096,
            "quantization_bits": 4,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.11303609798885282,
            "activation_std": 0.06089330437397921,
            "sparsity": 0.19185949053981607,
            "magnitude": 0.09788879835589068,
            "quantization_bits": 4,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.6289451807252622,
            "head_specialization": 0.7855559361512803,
            "reasoning_score": 0.6114662997590598
          },
          "16": {
            "entropy": 2.38683670535191,
            "head_specialization": 0.38136504423167633,
            "reasoning_score": 0.6475459322178869
          },
          "24": {
            "entropy": 1.5631994858858809,
            "head_specialization": 0.8419963796205152,
            "reasoning_score": 0.9072280863205407
          },
          "31": {
            "entropy": 2.374786441445595,
            "head_specialization": 0.818361684771658,
            "reasoning_score": 0.95139813217265
          }
        }
      }
    ]
  },
  "ARC_2bit": {
    "dataset": "ARC",
    "bits": 2,
    "accuracy": 0.05,
    "correct": 1,
    "total": 20,
    "time": 0.0009865760803222656,
    "device": "cpu",
    "critical_layers_affected": [
      24
    ],
    "layer_analysis_summary": {
      "0": {
        "sparsity": 0.04384014755006654,
        "critical": false
      },
      "8": {
        "sparsity": 0.1202048908669858,
        "critical": false
      },
      "16": {
        "sparsity": 0.2968657341960331,
        "critical": true
      },
      "24": {
        "sparsity": 0.7806620318013091,
        "critical": true
      },
      "31": {
        "sparsity": 0.24090042620456903,
        "critical": true
      }
    },
    "attention_summary": {
      "8": {
        "entropy": 0.6820955383146606,
        "head_specialization": 0.5068011402477416,
        "reasoning_score": 0.29376793618879565
      },
      "16": {
        "entropy": 1.9761380331428584,
        "head_specialization": 0.3991934809536301,
        "reasoning_score": 0.26027423305258945
      },
      "24": {
        "entropy": 0.16811980255753284,
        "head_specialization": 0.6161566965845858,
        "reasoning_score": 0.438705189376666
      },
      "31": {
        "entropy": 1.1195848745915038,
        "head_specialization": 0.5495112309552126,
        "reasoning_score": 0.35675289640391555
      }
    },
    "detailed_results": [
      {
        "sample": 0,
        "question": "Which factor will most likely cause a person to develop a fever?",
        "expected": "B",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.10478420181505643,
            "activation_std": 0.12885894940821785,
            "sparsity": 0.5347777209862086,
            "magnitude": 0.3581623135850049,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.060510119398521856,
            "activation_std": 0.3550374109853927,
            "sparsity": 0.17534703147073055,
            "magnitude": 0.0884914464203038,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.041836179296908235,
            "activation_std": 0.06333122455995116,
            "sparsity": 0.2832606820490287,
            "magnitude": 0.2556025492734522,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.10076667892437272,
            "activation_std": 0.43291477588817195,
            "sparsity": 0.1656326110563816,
            "magnitude": 1.03763832120893,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.16053992103979425,
            "activation_std": 0.24581682459154855,
            "sparsity": 0.5573115200433486,
            "magnitude": 0.21047219296524072,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.8570786948783843,
            "head_specialization": 0.4081916608158184,
            "reasoning_score": 0.45882455622144236
          },
          "16": {
            "entropy": 1.4922936514252272,
            "head_specialization": 0.26752704495746565,
            "reasoning_score": 0.37772585585272894
          },
          "24": {
            "entropy": 1.764415139146685,
            "head_specialization": 0.46424523940480406,
            "reasoning_score": 0.28654606431265167
          },
          "31": {
            "entropy": 0.7400287206812773,
            "head_specialization": 0.43100529275311245,
            "reasoning_score": 0.38152301675624706
          }
        }
      },
      {
        "sample": 1,
        "question": "Lichens are symbiotic organisms made of green algae and fungi. What do the green algae supply to the...",
        "expected": "B",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.0017507544793359284,
            "activation_std": 0.14388825353473061,
            "sparsity": 0.15086301956158066,
            "magnitude": 0.4007304204495495,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.023611990684559686,
            "activation_std": 0.1319250924209014,
            "sparsity": 0.3057058146768258,
            "magnitude": 0.4377852699522326,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.12263829000682054,
            "activation_std": 0.154976269186793,
            "sparsity": 0.30711674638904785,
            "magnitude": 1.596902089984679,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.2773269646341638,
            "activation_std": 0.0900485642272162,
            "sparsity": 0.5433624014293715,
            "magnitude": 0.037067599689570865,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.25183919237495134,
            "activation_std": 0.188615677782511,
            "sparsity": 0.10402743347410781,
            "magnitude": 1.1722364209090574,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.209186112562816,
            "head_specialization": 0.3028772592699426,
            "reasoning_score": 0.39596990470864335
          },
          "16": {
            "entropy": 2.7371570315524325,
            "head_specialization": 0.6429992238791858,
            "reasoning_score": 0.31251502433114114
          },
          "24": {
            "entropy": 1.4330909625638086,
            "head_specialization": 0.5357613487104061,
            "reasoning_score": 0.4510314979979979
          },
          "31": {
            "entropy": 1.5934642063224564,
            "head_specialization": 0.45430925514305454,
            "reasoning_score": 0.5594401277792088
          }
        }
      },
      {
        "sample": 2,
        "question": "When a switch is used in an electrical circuit, the switch can",
        "expected": "D",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.10767147102313859,
            "activation_std": 0.24706890955533087,
            "sparsity": 0.2826428361824974,
            "magnitude": 0.13567680382293804,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.14822039808558019,
            "activation_std": 0.15233548536141633,
            "sparsity": 0.27981890501998585,
            "magnitude": 0.4103818452121593,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.14872313472687618,
            "activation_std": 0.2860117256607272,
            "sparsity": 0.3563836317798368,
            "magnitude": 3.1462201004233505,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.24948333345503068,
            "activation_std": 0.18744630963155015,
            "sparsity": 0.35221538318067935,
            "magnitude": 0.6000617183786048,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.05537799372940351,
            "activation_std": 0.13858532683481292,
            "sparsity": 0.39699283336928737,
            "magnitude": 0.24949595244152561,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.792288377367157,
            "head_specialization": 0.5652654188098655,
            "reasoning_score": 0.37177849717561956
          },
          "16": {
            "entropy": 1.0588591703834342,
            "head_specialization": 0.49239460195571166,
            "reasoning_score": 0.5560655313828083
          },
          "24": {
            "entropy": 3.765960640927794,
            "head_specialization": 0.617192881441729,
            "reasoning_score": 0.3977510127985782
          },
          "31": {
            "entropy": 1.3510676165588213,
            "head_specialization": 0.6122957800699701,
            "reasoning_score": 0.4471816422936676
          }
        }
      },
      {
        "sample": 3,
        "question": "Which of the following is an example of an assistive device?",
        "expected": "A",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.023201152405858506,
            "activation_std": 0.028143464048822953,
            "sparsity": 0.35255052559825734,
            "magnitude": 0.6747038635819282,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.07132537258539061,
            "activation_std": 0.14053156055445357,
            "sparsity": 0.4217750792253408,
            "magnitude": 1.2139176269095768,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.17972238151706374,
            "activation_std": 0.16215051457081658,
            "sparsity": 0.2283295355645287,
            "magnitude": 2.163843516483912,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.019961699183676677,
            "activation_std": 0.1717516828365172,
            "sparsity": 0.2161906537306259,
            "magnitude": 1.5171866037846258,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.007348476475343654,
            "activation_std": 0.11076632081762508,
            "sparsity": 0.23189583258833094,
            "magnitude": 3.25389266665078,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.5556268840499277,
            "head_specialization": 0.4948987936818398,
            "reasoning_score": 0.49835170127633494
          },
          "16": {
            "entropy": 1.6359754091809582,
            "head_specialization": 0.6170785651353644,
            "reasoning_score": 0.5339915108548557
          },
          "24": {
            "entropy": 1.114983193704977,
            "head_specialization": 0.5128506706745265,
            "reasoning_score": 0.5396593101184372
          },
          "31": {
            "entropy": 0.6417078710115741,
            "head_specialization": 0.23645013557312486,
            "reasoning_score": 0.44143948783020986
          }
        }
      },
      {
        "sample": 4,
        "question": "Rocks are classified as igneous, metamorphic, or sedimentary according to",
        "expected": "3",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.14259461269989787,
            "activation_std": 0.04362328133488572,
            "sparsity": 0.4342497940719009,
            "magnitude": 3.1677704203499815,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.22600336753222686,
            "activation_std": 0.028912465451089206,
            "sparsity": 0.42720837131210554,
            "magnitude": 1.792960763863344,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.11532507117330754,
            "activation_std": 0.13624948999927353,
            "sparsity": 0.17702264668456136,
            "magnitude": 1.8517658844517062,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.19838894099175305,
            "activation_std": 0.3297836020444942,
            "sparsity": 0.7694036116051755,
            "magnitude": 0.24661428290750148,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.29003712325351877,
            "activation_std": 0.6242530406544721,
            "sparsity": 0.24358867219140157,
            "magnitude": 2.297332030530143,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 5.560472343657782,
            "head_specialization": 0.4677980630246088,
            "reasoning_score": 0.49658078303292236
          },
          "16": {
            "entropy": 1.1809825910870086,
            "head_specialization": 0.41141320237272133,
            "reasoning_score": 0.4190183334260675
          },
          "24": {
            "entropy": 2.1573256648136487,
            "head_specialization": 0.45860619897434934,
            "reasoning_score": 0.5301208963564341
          },
          "31": {
            "entropy": 0.7725368565440199,
            "head_specialization": 0.6426485017547529,
            "reasoning_score": 0.3283741581661242
          }
        }
      },
      {
        "sample": 5,
        "question": "A chewable calcium carbonate tablet is a common treatment for stomach discomfort. Calcium carbonate ...",
        "expected": "C",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.1161452697025834,
            "activation_std": 0.2570633137207423,
            "sparsity": 0.251502408611587,
            "magnitude": 2.3970041865578544,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.07815762483924721,
            "activation_std": 0.7518259923156689,
            "sparsity": 0.4715434987236957,
            "magnitude": 0.5227718968989711,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.048443665121474584,
            "activation_std": 0.08602683891051499,
            "sparsity": 0.14413363483382305,
            "magnitude": 0.32852311849249854,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.06016732062482008,
            "activation_std": 0.06617217131009566,
            "sparsity": 0.07688329343033751,
            "magnitude": 0.26773337930921326,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.18876584607273614,
            "activation_std": 0.016852637119263554,
            "sparsity": 0.0616967373522169,
            "magnitude": 0.13907239505464186,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 4.525650406190833,
            "head_specialization": 0.2848121121507365,
            "reasoning_score": 0.33477100428558976
          },
          "16": {
            "entropy": 1.668429339278283,
            "head_specialization": 0.6137469268382304,
            "reasoning_score": 0.41560483291395234
          },
          "24": {
            "entropy": 1.928952292907148,
            "head_specialization": 0.3792831671540162,
            "reasoning_score": 0.34663005434283706
          },
          "31": {
            "entropy": 2.5654614718428794,
            "head_specialization": 0.3483274298462482,
            "reasoning_score": 0.5909893408314196
          }
        }
      },
      {
        "sample": 6,
        "question": "Which two body systems are directly involved in movement?",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.1329401807587979,
            "activation_std": 0.18861779607311205,
            "sparsity": 0.44343980268309574,
            "magnitude": 0.0702814218420864,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.16243048149354,
            "activation_std": 0.09287450732913732,
            "sparsity": 0.15444289589972415,
            "magnitude": 2.1887912894136785,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.08404148828388577,
            "activation_std": 0.39897694086266455,
            "sparsity": 0.29706953752995746,
            "magnitude": 0.13454058159551682,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.06417719661720382,
            "activation_std": 0.08399987525109659,
            "sparsity": 0.2783842703040296,
            "magnitude": 0.1875968819812616,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.040872358483343835,
            "activation_std": 0.03788592226488785,
            "sparsity": 0.3362483560742762,
            "magnitude": 0.024989730057125844,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.9923161958975364,
            "head_specialization": 0.5539655997047398,
            "reasoning_score": 0.34944678941113305
          },
          "16": {
            "entropy": 1.3553638904504004,
            "head_specialization": 0.5219892794823373,
            "reasoning_score": 0.6030972963131253
          },
          "24": {
            "entropy": 3.5698475076464473,
            "head_specialization": 0.4843359330380938,
            "reasoning_score": 0.5045477753887548
          },
          "31": {
            "entropy": 1.9543350299049704,
            "head_specialization": 0.457691678631368,
            "reasoning_score": 0.47682387665622455
          }
        }
      },
      {
        "sample": 7,
        "question": "Which change in the state of water particles causes the particles to become arranged in a fixed posi...",
        "expected": "C",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.02121218650611122,
            "activation_std": 0.47062566934996297,
            "sparsity": 0.2669678685468062,
            "magnitude": 3.8670432102132937,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.08318881433100334,
            "activation_std": 0.1499607462554383,
            "sparsity": 0.1323572813104104,
            "magnitude": 0.41743327747934067,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.04127458304614893,
            "activation_std": 0.20950816589803845,
            "sparsity": 0.5841802839157668,
            "magnitude": 1.243405505403052,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.09594814081569167,
            "activation_std": 0.2743139480469702,
            "sparsity": 0.3455452749425104,
            "magnitude": 1.824856234381203,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.0877530872783931,
            "activation_std": 0.20321641553540307,
            "sparsity": 0.31688033489028333,
            "magnitude": 0.8103594985923801,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.559399482501651,
            "head_specialization": 0.4263201794111626,
            "reasoning_score": 0.3982502242649064
          },
          "16": {
            "entropy": 3.1846907355657725,
            "head_specialization": 0.4221546117843256,
            "reasoning_score": 0.49056127747628686
          },
          "24": {
            "entropy": 3.1477464603795653,
            "head_specialization": 0.47117305156080874,
            "reasoning_score": 0.3884264443449868
          },
          "31": {
            "entropy": 1.0396032912735853,
            "head_specialization": 0.6293210819179172,
            "reasoning_score": 0.4724361150971195
          }
        }
      },
      {
        "sample": 8,
        "question": "Earth's core is primarily composed of which of the following materials?",
        "expected": "B",
        "generated": "B is correct",
        "correct": true,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.09032106859050446,
            "activation_std": 0.20337459677202674,
            "sparsity": 0.19248498715500734,
            "magnitude": 1.1411970480585456,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.0018355095158931417,
            "activation_std": 0.09996974333486223,
            "sparsity": 0.544507776158144,
            "magnitude": 1.244735786432976,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.10398570585861798,
            "activation_std": 0.03226072008667062,
            "sparsity": 0.4372074446098386,
            "magnitude": 0.10175639771824192,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.18161935529822298,
            "activation_std": 0.24224118196352365,
            "sparsity": 0.10918689586216149,
            "magnitude": 0.7183680513898587,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.11433010364411803,
            "activation_std": 0.12113663448810968,
            "sparsity": 0.1887528858552574,
            "magnitude": 2.0569898382090304,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.8464106722319745,
            "head_specialization": 0.41321431216183035,
            "reasoning_score": 0.3514329975184858
          },
          "16": {
            "entropy": 2.998129881450672,
            "head_specialization": 0.30668668699298185,
            "reasoning_score": 0.46550337044435225
          },
          "24": {
            "entropy": 5.673118191117478,
            "head_specialization": 0.5430364451952322,
            "reasoning_score": 0.551884543822459
          },
          "31": {
            "entropy": 3.1975100249669177,
            "head_specialization": 0.47078963252257294,
            "reasoning_score": 0.5523477711740662
          }
        }
      },
      {
        "sample": 9,
        "question": "Which of the following events during meiosis contributes most to the variation within a species?",
        "expected": "C",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.2274553113865665,
            "activation_std": 0.022616421647101134,
            "sparsity": 0.1997533826677228,
            "magnitude": 1.509410624824803,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.0536121690678649,
            "activation_std": 0.0778012356208959,
            "sparsity": 0.06187591955101078,
            "magnitude": 0.9054401001958473,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.04216366424423144,
            "activation_std": 0.29587980560682675,
            "sparsity": 0.2258752034044653,
            "magnitude": 0.49233731497584826,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.05796133600066956,
            "activation_std": 0.13964102923435387,
            "sparsity": 0.10994304886414576,
            "magnitude": 0.504842568363913,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.03424488615133585,
            "activation_std": 0.10140943473690035,
            "sparsity": 0.15295032615766688,
            "magnitude": 0.6272717726054517,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.5390648949298305,
            "head_specialization": 0.4118384693490986,
            "reasoning_score": 0.42259275557718495
          },
          "16": {
            "entropy": 2.4735338793110513,
            "head_specialization": 0.41760594160217424,
            "reasoning_score": 0.4131595967012056
          },
          "24": {
            "entropy": 3.7723407944059977,
            "head_specialization": 0.5615093113188151,
            "reasoning_score": 0.42622018075870044
          },
          "31": {
            "entropy": 0.8441445171053863,
            "head_specialization": 0.5151165359934565,
            "reasoning_score": 0.4250980893471343
          }
        }
      },
      {
        "sample": 10,
        "question": "Which of the following was probably most important in the formation of dark, fertile soil that is go...",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.010655148155251587,
            "activation_std": 0.08355303694750726,
            "sparsity": 0.18591241899192407,
            "magnitude": 1.9429659678847202,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.12535384953368034,
            "activation_std": 0.17236757842754655,
            "sparsity": 0.05935524232149783,
            "magnitude": 0.4383613340329978,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.13744522568792075,
            "activation_std": 0.1301746321640039,
            "sparsity": 0.10962616178089576,
            "magnitude": 0.44434954576793806,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.04493583025984513,
            "activation_std": 0.1007130199525012,
            "sparsity": 0.19099839987914038,
            "magnitude": 1.5583250742663046,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.06899932128913722,
            "activation_std": 0.0829764224807944,
            "sparsity": 0.597334142983865,
            "magnitude": 1.171247653380745,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.4250988235738893,
            "head_specialization": 0.5387223594549359,
            "reasoning_score": 0.5472509253443487
          },
          "16": {
            "entropy": 1.0921789666279405,
            "head_specialization": 0.43822517806597294,
            "reasoning_score": 0.625868441326769
          },
          "24": {
            "entropy": 2.7389725553637736,
            "head_specialization": 0.48097034538474265,
            "reasoning_score": 0.4621266140929239
          },
          "31": {
            "entropy": 1.1379566098731972,
            "head_specialization": 0.47020675210093016,
            "reasoning_score": 0.3797842339743835
          }
        }
      },
      {
        "sample": 11,
        "question": "When igneous rock is changed into metamorphic rock, which form of energy is this process?",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.020511056932532024,
            "activation_std": 0.05318690867324859,
            "sparsity": 0.289629257381228,
            "magnitude": 1.6121793573738177,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.18424337776775848,
            "activation_std": 0.13107132212597042,
            "sparsity": 0.16024073824779578,
            "magnitude": 0.3597363240052767,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.07764288084656731,
            "activation_std": 0.28691350672436156,
            "sparsity": 0.2787222713508937,
            "magnitude": 0.16473636643300668,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.028866000294498092,
            "activation_std": 0.35920763572449677,
            "sparsity": 0.260467723131934,
            "magnitude": 0.42767106313980047,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.1027197619729567,
            "activation_std": 0.13610318632150453,
            "sparsity": 0.14462882025995738,
            "magnitude": 0.18461055892496214,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 3.8618417335615627,
            "head_specialization": 0.10137127305039295,
            "reasoning_score": 0.5160637769528693
          },
          "16": {
            "entropy": 2.1067702289547294,
            "head_specialization": 0.5326611258231356,
            "reasoning_score": 0.46365782561767416
          },
          "24": {
            "entropy": 5.0155653231198665,
            "head_specialization": 0.5787261725967995,
            "reasoning_score": 0.39632359897162034
          },
          "31": {
            "entropy": 1.5160409171571652,
            "head_specialization": 0.32175076856320556,
            "reasoning_score": 0.5507787067375057
          }
        }
      },
      {
        "sample": 12,
        "question": "In humans, the digestion process begins in",
        "expected": "B",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.13466720870760876,
            "activation_std": 0.21541941464604308,
            "sparsity": 0.12198407457469959,
            "magnitude": 1.6971482973913639,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.14770850030212812,
            "activation_std": 0.14110421701266232,
            "sparsity": 0.2280590560127394,
            "magnitude": 0.5033702938523926,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.20799824180828702,
            "activation_std": 0.06666471051511805,
            "sparsity": 0.06955118708513584,
            "magnitude": 1.4368492487395357,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.017030446630285027,
            "activation_std": 0.09765396217150513,
            "sparsity": 0.20785884015795364,
            "magnitude": 1.3795253451153457,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.03155078235334242,
            "activation_std": 0.1357516365327823,
            "sparsity": 0.1146337346691031,
            "magnitude": 0.010329897596982305,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 3.4290499929333538,
            "head_specialization": 0.41891586699877154,
            "reasoning_score": 0.4802995184884644
          },
          "16": {
            "entropy": 0.4730063740394296,
            "head_specialization": 0.4217537731245112,
            "reasoning_score": 0.372853572754398
          },
          "24": {
            "entropy": 2.1541921810182365,
            "head_specialization": 0.6041322506674076,
            "reasoning_score": 0.4026871065799426
          },
          "31": {
            "entropy": 0.9427379829502155,
            "head_specialization": 0.4604351398940887,
            "reasoning_score": 0.2530315534601349
          }
        }
      },
      {
        "sample": 13,
        "question": "Which of these items contains only a solution?",
        "expected": "B",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.22027864428472052,
            "activation_std": 0.0902887953645778,
            "sparsity": 0.20729383070119875,
            "magnitude": 1.1238602745282897,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.1486113407764851,
            "activation_std": 0.12499548035022767,
            "sparsity": 0.2585467030643146,
            "magnitude": 0.21065033306331815,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.05026850106675345,
            "activation_std": 0.37708130833415693,
            "sparsity": 0.5307642427793053,
            "magnitude": 0.2802615095854222,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.175090356830661,
            "activation_std": 0.025235047532197106,
            "sparsity": 0.3203507590749163,
            "magnitude": 0.030563262290461742,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.25087989250948534,
            "activation_std": 0.32919268239531535,
            "sparsity": 0.009199839847947793,
            "magnitude": 0.6769055035504413,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 3.772655219168318,
            "head_specialization": 0.5689608058757724,
            "reasoning_score": 0.5499290116904578
          },
          "16": {
            "entropy": 1.5524015583197706,
            "head_specialization": 0.46407075502049927,
            "reasoning_score": 0.28942032190536876
          },
          "24": {
            "entropy": 2.470918445013765,
            "head_specialization": 0.4485679773278777,
            "reasoning_score": 0.5435723905040891
          },
          "31": {
            "entropy": 2.05987568265001,
            "head_specialization": 0.42705617036204696,
            "reasoning_score": 0.5216332368075937
          }
        }
      },
      {
        "sample": 14,
        "question": "Many natural rock formations change color over time. In Utah, for example, iron oxidized and formed ...",
        "expected": "A",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.055929300121574,
            "activation_std": 0.09529767340593022,
            "sparsity": 0.36441310109039493,
            "magnitude": 0.06545052333278115,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.2582555088068859,
            "activation_std": 0.08730625039632396,
            "sparsity": 0.3564328540182103,
            "magnitude": 1.4011950863018015,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.020327890933565892,
            "activation_std": 0.11467587780494058,
            "sparsity": 0.04474612294925989,
            "magnitude": 0.618728621906477,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.23020920585843038,
            "activation_std": 0.04093756039843069,
            "sparsity": 0.14158535207787684,
            "magnitude": 0.08934737301514636,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.1797669892957343,
            "activation_std": 0.09835635483567383,
            "sparsity": 0.3128531960074045,
            "magnitude": 0.7695304081381886,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.6672938255591387,
            "head_specialization": 0.4688703753151641,
            "reasoning_score": 0.14275650014146418
          },
          "16": {
            "entropy": 0.3937248482572241,
            "head_specialization": 0.5223518221554898,
            "reasoning_score": 0.4656656794207645
          },
          "24": {
            "entropy": 2.795109868134965,
            "head_specialization": 0.503970766650543,
            "reasoning_score": 0.24282030294257093
          },
          "31": {
            "entropy": 1.4132518940252674,
            "head_specialization": 0.6334662429651726,
            "reasoning_score": 0.5959623893934559
          }
        }
      },
      {
        "sample": 15,
        "question": "A population of small, plant-eating beetles lives in a forest. About half of the beetles are light b...",
        "expected": "A",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.14728428740864408,
            "activation_std": 0.3234528520621487,
            "sparsity": 0.2817819345259274,
            "magnitude": 0.1905466902092914,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.07990099863370421,
            "activation_std": 0.07820078351155628,
            "sparsity": 0.1574058925413448,
            "magnitude": 0.6683959518157516,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.08216928067704513,
            "activation_std": 0.16976948537179776,
            "sparsity": 0.18181752504187093,
            "magnitude": 2.1646849288994066,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.08056595020865435,
            "activation_std": 0.11630615781849928,
            "sparsity": 0.7693576689201753,
            "magnitude": 0.9890521755566694,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.08228504764230411,
            "activation_std": 0.3567723132143762,
            "sparsity": 0.16900995609144537,
            "magnitude": 1.619525444010681,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.44003599049825093,
            "head_specialization": 0.5364898209016136,
            "reasoning_score": 0.30269911135886846
          },
          "16": {
            "entropy": 1.2917737270468208,
            "head_specialization": 0.41214240969826066,
            "reasoning_score": 0.20114506213152525
          },
          "24": {
            "entropy": 6.330453012393159,
            "head_specialization": 0.3712632760996828,
            "reasoning_score": 0.43858242666386055
          },
          "31": {
            "entropy": 2.1779414340977246,
            "head_specialization": 0.523410235653111,
            "reasoning_score": 0.23462593036577428
          }
        }
      },
      {
        "sample": 16,
        "question": "A scientist wanting to document a change in a river's flow pattern should observe a river over a per...",
        "expected": "D",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.03049340661856422,
            "activation_std": 0.1606223279144467,
            "sparsity": 0.4917838437703923,
            "magnitude": 0.0663709687974279,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.10087372908592813,
            "activation_std": 0.14173934412343558,
            "sparsity": 0.15434467534414703,
            "magnitude": 0.849836525652781,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.008014271977396438,
            "activation_std": 0.21051603125592694,
            "sparsity": 0.17420521653919485,
            "magnitude": 0.8633541483866725,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.05052298680783653,
            "activation_std": 0.12465279813051294,
            "sparsity": 0.2865132519255509,
            "magnitude": 0.08945445939239277,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.06636859108911394,
            "activation_std": 0.4171416195539256,
            "sparsity": 0.07931761122186902,
            "magnitude": 3.0272552319612176,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.6783618722943423,
            "head_specialization": 0.5853565102794732,
            "reasoning_score": 0.3635518577404306
          },
          "16": {
            "entropy": 2.8873750423486055,
            "head_specialization": 0.5227182675096722,
            "reasoning_score": 0.5810237261385237
          },
          "24": {
            "entropy": 1.2578219751987703,
            "head_specialization": 0.43230155795416486,
            "reasoning_score": 0.4248470916726929
          },
          "31": {
            "entropy": 1.6855126253331019,
            "head_specialization": 0.4247261839560294,
            "reasoning_score": 0.3567638596802141
          }
        }
      },
      {
        "sample": 17,
        "question": "Automobile engines built today are designed to be gas efficient. Gas-efficient engines most likely a...",
        "expected": "A",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.08061055725173127,
            "activation_std": 0.059769329715632945,
            "sparsity": 0.33394603878602563,
            "magnitude": 0.760693178365726,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.19473447700595395,
            "activation_std": 0.2091119064335149,
            "sparsity": 0.08858939447397153,
            "magnitude": 0.8600323198966822,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.005373560470907512,
            "activation_std": 0.1803082561976052,
            "sparsity": 0.279308580897734,
            "magnitude": 1.6866709833855953,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.0035258656542426657,
            "activation_std": 0.4021938102797917,
            "sparsity": 0.585446417855773,
            "magnitude": 0.5116423704841595,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.023009440003269908,
            "activation_std": 0.11246010245640395,
            "sparsity": 0.2932289127841686,
            "magnitude": 0.15614077515681524,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.316859194035783,
            "head_specialization": 0.43300499707279494,
            "reasoning_score": 0.5327643485544757
          },
          "16": {
            "entropy": 2.8985506949045647,
            "head_specialization": 0.6167651143524618,
            "reasoning_score": 0.5172350845658454
          },
          "24": {
            "entropy": 1.6141720000045137,
            "head_specialization": 0.5653765381008232,
            "reasoning_score": 0.5558409387414849
          },
          "31": {
            "entropy": 2.9193080912965548,
            "head_specialization": 0.6348439688359198,
            "reasoning_score": 0.4117519595462071
          }
        }
      },
      {
        "sample": 18,
        "question": "A student in an empty classroom shouts, \"Hello!\" Which best explains what the student hears after th...",
        "expected": "B",
        "generated": "The answer is A",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.03800118895776516,
            "activation_std": 0.038206939596503994,
            "sparsity": 0.02097973025357795,
            "magnitude": 0.6183299873512382,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.29745880769995975,
            "activation_std": 0.25715401018337036,
            "sparsity": 0.37069994700085235,
            "magnitude": 2.7576340897508933,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.3727716412889767,
            "activation_std": 0.17800271452216312,
            "sparsity": 0.32173063928774037,
            "magnitude": 1.4330301116588775,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.2049820056129355,
            "activation_std": 0.05474738853622882,
            "sparsity": 0.5249368549199714,
            "magnitude": 0.26778884253469043,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.2189535411347393,
            "activation_std": 0.08571817309608246,
            "sparsity": 0.42567037014209064,
            "magnitude": 0.7962680057739725,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 4.21802792177883,
            "head_specialization": 0.4121903790722943,
            "reasoning_score": 0.43252887588719563
          },
          "16": {
            "entropy": 4.518479533422295,
            "head_specialization": 0.5677465773167313,
            "reasoning_score": 0.4455733622188159
          },
          "24": {
            "entropy": 3.700715843874056,
            "head_specialization": 0.3982271718463948,
            "reasoning_score": 0.5906476168389074
          },
          "31": {
            "entropy": 1.3511459830113832,
            "head_specialization": 0.2715995018630818,
            "reasoning_score": 0.5540501152407539
          }
        }
      },
      {
        "sample": 19,
        "question": "Which type of energy in gasoline is transformed into mechanical energy in a motorcycle engine?",
        "expected": "1",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.15473916495220652,
            "activation_std": 0.24396396753208247,
            "sparsity": 0.04384014755006654,
            "magnitude": 0.4683276055477201,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.29086011643743026,
            "activation_std": 0.3727456785582937,
            "sparsity": 0.1202048908669858,
            "magnitude": 2.319967606108364,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.044062866958680495,
            "activation_std": 0.013549345747877455,
            "sparsity": 0.2968657341960331,
            "magnitude": 3.112409400101603,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.08433332997763293,
            "activation_std": 0.11543535411276237,
            "sparsity": 0.7806620318013091,
            "magnitude": 1.5031908414249573,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.22928788887102247,
            "activation_std": 0.14641800656435844,
            "sparsity": 0.24090042620456903,
            "magnitude": 0.7662544191916846,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.6820955383146606,
            "head_specialization": 0.5068011402477416,
            "reasoning_score": 0.29376793618879565
          },
          "16": {
            "entropy": 1.9761380331428584,
            "head_specialization": 0.3991934809536301,
            "reasoning_score": 0.26027423305258945
          },
          "24": {
            "entropy": 0.16811980255753284,
            "head_specialization": 0.6161566965845858,
            "reasoning_score": 0.438705189376666
          },
          "31": {
            "entropy": 1.1195848745915038,
            "head_specialization": 0.5495112309552126,
            "reasoning_score": 0.35675289640391555
          }
        }
      }
    ]
  },
  "SVAMP_2bit": {
    "dataset": "SVAMP",
    "bits": 2,
    "accuracy": 0.05,
    "correct": 1,
    "total": 20,
    "time": 0.0009655952453613281,
    "device": "cpu",
    "critical_layers_affected": [],
    "layer_analysis_summary": {
      "0": {
        "sparsity": 0.3842288336287984,
        "critical": false
      },
      "8": {
        "sparsity": 0.10589251965758523,
        "critical": false
      },
      "16": {
        "sparsity": 0.290475863240304,
        "critical": true
      },
      "24": {
        "sparsity": 0.14376812831109545,
        "critical": true
      },
      "31": {
        "sparsity": 0.21791422587405618,
        "critical": true
      }
    },
    "attention_summary": {
      "8": {
        "entropy": 1.9550668174011525,
        "head_specialization": 0.3303443305326623,
        "reasoning_score": 0.4101693307284004
      },
      "16": {
        "entropy": 2.850634338574155,
        "head_specialization": 0.2858254423832135,
        "reasoning_score": 0.5169914858853862
      },
      "24": {
        "entropy": 1.4718800754354338,
        "head_specialization": 0.4581055638884202,
        "reasoning_score": 0.4593494824857101
      },
      "31": {
        "entropy": 1.1009945854961156,
        "head_specialization": 0.5392703630789414,
        "reasoning_score": 0.1323505334783506
      }
    },
    "detailed_results": [
      {
        "sample": 0,
        "question": "There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 gro...",
        "expected": "145",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.16322310558977202,
            "activation_std": 0.1682781370749356,
            "sparsity": 0.10553820375462741,
            "magnitude": 0.9879386281674639,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.24283903090863654,
            "activation_std": 0.05663945715361919,
            "sparsity": 0.15040526524813586,
            "magnitude": 0.5910802011316281,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.25166820378233096,
            "activation_std": 0.34846499485042126,
            "sparsity": 0.4929560639741327,
            "magnitude": 0.9505060101581405,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.21923025550002395,
            "activation_std": 0.24877915773586934,
            "sparsity": 0.22776045340726908,
            "magnitude": 0.5257854268772224,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.3785392262564263,
            "activation_std": 0.33087192712826563,
            "sparsity": 0.41742226916408726,
            "magnitude": 0.2487120733379081,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.3807574776192022,
            "head_specialization": 0.34198518716266507,
            "reasoning_score": 0.14617873398488943
          },
          "16": {
            "entropy": 1.8905256336391778,
            "head_specialization": 0.32728658705222574,
            "reasoning_score": 0.5459142630310619
          },
          "24": {
            "entropy": 1.5980130705335047,
            "head_specialization": 0.2027453479331447,
            "reasoning_score": 0.5652672033111474
          },
          "31": {
            "entropy": 2.9590841395139638,
            "head_specialization": 0.5177327652038178,
            "reasoning_score": 0.3100327307702886
          }
        }
      },
      {
        "sample": 1,
        "question": "Marco and his dad went strawberry picking. Marco's dad's strawberries weighed 11 pounds. If together...",
        "expected": "19",
        "generated": "B is correct",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.16845673121795146,
            "activation_std": 0.04309727582733191,
            "sparsity": 0.47198513950786314,
            "magnitude": 0.8220105748561051,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.13761862625794402,
            "activation_std": 0.20102332362183106,
            "sparsity": 0.08095503088989348,
            "magnitude": 0.2846503197368422,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.2978814864415111,
            "activation_std": 0.04225303994505848,
            "sparsity": 0.37630751638233706,
            "magnitude": 0.2013216864996763,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.2755526350570845,
            "activation_std": 0.19537698432980077,
            "sparsity": 0.13594513729500757,
            "magnitude": 0.7608544053528112,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.015254244355481153,
            "activation_std": 0.0735498249286262,
            "sparsity": 0.42626779353712263,
            "magnitude": 0.013208153081838593,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.927200265940587,
            "head_specialization": 0.5236463912828021,
            "reasoning_score": 0.5347736249943301
          },
          "16": {
            "entropy": 3.953943337855695,
            "head_specialization": 0.4964947459500806,
            "reasoning_score": 0.579924880019712
          },
          "24": {
            "entropy": 0.3944104558663799,
            "head_specialization": 0.5389338380358957,
            "reasoning_score": 0.5633599611947008
          },
          "31": {
            "entropy": 2.532867594582534,
            "head_specialization": 0.48509625746176194,
            "reasoning_score": 0.4688907770647696
          }
        }
      },
      {
        "sample": 2,
        "question": "Edward spent $ 6 to buy 2 books each book costing him the same amount of money. Now he has $ 12. How...",
        "expected": "3",
        "generated": "3",
        "correct": true,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.05458396794480607,
            "activation_std": 0.14495961427413584,
            "sparsity": 0.24192485170098368,
            "magnitude": 0.002943482616784001,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.38028944852722696,
            "activation_std": 0.18241433870033455,
            "sparsity": 0.44678869115590175,
            "magnitude": 1.3635621903249595,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.06063879793327988,
            "activation_std": 0.15525468912187057,
            "sparsity": 0.2324732736453857,
            "magnitude": 0.6173286211119566,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.11116819174358833,
            "activation_std": 0.0551630966651811,
            "sparsity": 0.21761830203278523,
            "magnitude": 0.43094447047490825,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.15043963658278448,
            "activation_std": 0.16674178805086787,
            "sparsity": 0.9543641891173332,
            "magnitude": 2.1812443179223715,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 3.5016131002060926,
            "head_specialization": 0.46216614891673863,
            "reasoning_score": 0.4576702064110371
          },
          "16": {
            "entropy": 0.821950450941646,
            "head_specialization": 0.60979622784456,
            "reasoning_score": 0.5798138659707824
          },
          "24": {
            "entropy": 3.8315355090174705,
            "head_specialization": 0.4954009935772541,
            "reasoning_score": 0.6235579618570025
          },
          "31": {
            "entropy": 2.5038332965248093,
            "head_specialization": 0.4586734464898603,
            "reasoning_score": 0.44863627362568576
          }
        }
      },
      {
        "sample": 3,
        "question": "Frank was reading through his favorite book. The book had 3 chapters, each with the same number of p...",
        "expected": "198",
        "generated": "Choice C",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.09184552938518772,
            "activation_std": 0.22109596092979703,
            "sparsity": 0.11014494196495245,
            "magnitude": 0.139726431323214,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.008732251140662489,
            "activation_std": 0.14345852798466902,
            "sparsity": 0.34184478504653315,
            "magnitude": 2.465751884116643,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.09645859026302346,
            "activation_std": 0.18984837608566915,
            "sparsity": 0.5951234284352962,
            "magnitude": 0.4424984384642102,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.0748988403665779,
            "activation_std": 0.26008870202179596,
            "sparsity": 0.09137631052904877,
            "magnitude": 2.9548775791589743,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.12092582955653929,
            "activation_std": 0.1538681439304741,
            "sparsity": 0.2447522681555768,
            "magnitude": 0.4141133096453792,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 3.676856601818756,
            "head_specialization": 0.40598007246697637,
            "reasoning_score": 0.30616619371911874
          },
          "16": {
            "entropy": 0.6158089862901999,
            "head_specialization": 0.5350792289817782,
            "reasoning_score": 0.5135266323538351
          },
          "24": {
            "entropy": 2.660992686431028,
            "head_specialization": 0.49670414328454365,
            "reasoning_score": 0.39552104689565765
          },
          "31": {
            "entropy": 0.731463916492832,
            "head_specialization": 0.4755718967993732,
            "reasoning_score": 0.23804558295720368
          }
        }
      },
      {
        "sample": 4,
        "question": "There were 78 dollars in Olivia's wallet. She spent 15 dollars at a supermarket. How much money does...",
        "expected": "63",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.24658046566059688,
            "activation_std": 0.08052686153231334,
            "sparsity": 0.30913136113171724,
            "magnitude": 0.27791990021019486,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.019011550657801583,
            "activation_std": 0.29407080591158286,
            "sparsity": 0.2539465339014767,
            "magnitude": 0.21741408610445265,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.029567117659634386,
            "activation_std": 0.15984691923659525,
            "sparsity": 0.22455870583839388,
            "magnitude": 0.3477723949426948,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.0931762067953308,
            "activation_std": 0.16162964047563713,
            "sparsity": 0.0480966142022946,
            "magnitude": 0.9279228961082553,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.06211359710205209,
            "activation_std": 0.37358308670760726,
            "sparsity": 0.07666770716151824,
            "magnitude": 0.39552652568426044,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 3.7534230151098766,
            "head_specialization": 0.558933356705312,
            "reasoning_score": 0.2900280724875732
          },
          "16": {
            "entropy": 1.199317910870127,
            "head_specialization": 0.6431089761766674,
            "reasoning_score": 0.287176004706233
          },
          "24": {
            "entropy": 5.603127273193071,
            "head_specialization": 0.5035127666781466,
            "reasoning_score": 0.5226072271890067
          },
          "31": {
            "entropy": 5.009853735208167,
            "head_specialization": 0.5337058547013369,
            "reasoning_score": 0.4855575473638634
          }
        }
      },
      {
        "sample": 5,
        "question": "Paul got a box of 110 crayons for his birthday. During the school year he gave 90 crayons to his fri...",
        "expected": "322",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.08593161097292744,
            "activation_std": 0.08273968544365476,
            "sparsity": 0.4567629346692368,
            "magnitude": 0.292133033448055,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.0589352730694777,
            "activation_std": 0.21036413530704953,
            "sparsity": 0.17253044836437909,
            "magnitude": 0.7805079509277304,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.24976718703304168,
            "activation_std": 0.1427510606559877,
            "sparsity": 0.3683698045448218,
            "magnitude": 2.7584995447050558,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.17113219716799097,
            "activation_std": 0.16203963396633836,
            "sparsity": 0.07431707434719306,
            "magnitude": 0.974000146286086,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.02139601794788437,
            "activation_std": 0.0744513738127672,
            "sparsity": 0.08972308222406705,
            "magnitude": 1.579016069511183,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.8174789797524684,
            "head_specialization": 0.4685223615151313,
            "reasoning_score": 0.6181895555179083
          },
          "16": {
            "entropy": 1.0156236210227865,
            "head_specialization": 0.3380924845656841,
            "reasoning_score": 0.5284872512470419
          },
          "24": {
            "entropy": 0.8132565245975907,
            "head_specialization": 0.5186075393016755,
            "reasoning_score": 0.41033870655758187
          },
          "31": {
            "entropy": 1.7889586236375141,
            "head_specialization": 0.3862007400775438,
            "reasoning_score": 0.4977754998882338
          }
        }
      },
      {
        "sample": 6,
        "question": "Randy has 95 blocks. He uses 20 blocks to build a house and 50 blocks to build a tower. How many mor...",
        "expected": "30",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.07145461851516012,
            "activation_std": 0.06399784073527917,
            "sparsity": 0.24966339167515816,
            "magnitude": 1.8095689442159835,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.15607774192839569,
            "activation_std": 0.011579453509637981,
            "sparsity": 0.2115594377720577,
            "magnitude": 0.960555547980201,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.18943968289146812,
            "activation_std": 0.3926373521165142,
            "sparsity": 0.39400355298186907,
            "magnitude": 0.030271100766710725,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.0918352840620639,
            "activation_std": 0.04008587712742926,
            "sparsity": 0.32646405820006813,
            "magnitude": 2.0111770310701877,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.1300403801424069,
            "activation_std": 0.008785685215544608,
            "sparsity": 0.49841682116950686,
            "magnitude": 2.8959202944030973,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.067318285060844,
            "head_specialization": 0.5398013920696022,
            "reasoning_score": 0.5218030952690955
          },
          "16": {
            "entropy": 3.3516331307276026,
            "head_specialization": 0.4287862546308587,
            "reasoning_score": 0.5980613043961743
          },
          "24": {
            "entropy": 0.5824552866577286,
            "head_specialization": 0.4753681561282199,
            "reasoning_score": 0.5732713061253132
          },
          "31": {
            "entropy": 2.3327515330544824,
            "head_specialization": 0.45815696649344956,
            "reasoning_score": 0.4183006715976488
          }
        }
      },
      {
        "sample": 7,
        "question": "After Jessie started to go jogging everyday she lost 126 kilograms. She currently weighs 66 kilogram...",
        "expected": "192",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.06436707928980781,
            "activation_std": 0.18788831738355888,
            "sparsity": 0.40574394805544445,
            "magnitude": 0.1959766643311046,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.21555152780571338,
            "activation_std": 0.028403520473255436,
            "sparsity": 0.42491024030537705,
            "magnitude": 0.6942356272297484,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.1524846731851352,
            "activation_std": 0.1235717157473582,
            "sparsity": 0.06952508685285452,
            "magnitude": 0.3302826501181938,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.1829164598433076,
            "activation_std": 0.2256519809128387,
            "sparsity": 0.07062334287950729,
            "magnitude": 0.2482751715854818,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.000964826002447607,
            "activation_std": 0.07979702998841932,
            "sparsity": 0.16675681268158893,
            "magnitude": 0.08040692533807778,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.911732669389262,
            "head_specialization": 0.4509568246911191,
            "reasoning_score": 0.5429129169964947
          },
          "16": {
            "entropy": 0.9768245270895493,
            "head_specialization": 0.6174438706914507,
            "reasoning_score": 0.5120565756177851
          },
          "24": {
            "entropy": 3.1790359871567473,
            "head_specialization": 0.5865061564311563,
            "reasoning_score": 0.31655741536525467
          },
          "31": {
            "entropy": 1.7109932261111556,
            "head_specialization": 0.6237220717430548,
            "reasoning_score": 0.35158634843165504
          }
        }
      },
      {
        "sample": 8,
        "question": "At the arcade Dave had won some tickets. He used 12 tickets to buy some toys. If he still has 14 tic...",
        "expected": "26",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.08401627712507691,
            "activation_std": 0.1097108999642819,
            "sparsity": 0.3550655440663416,
            "magnitude": 0.9401104643616525,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.22754986435263957,
            "activation_std": 0.11134394715970866,
            "sparsity": 0.3057453168008406,
            "magnitude": 1.6733127534371168,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.1407160382848145,
            "activation_std": 0.050911800115255604,
            "sparsity": 0.018498664970740826,
            "magnitude": 0.9118939858685092,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.02099863627870886,
            "activation_std": 0.21739101343408074,
            "sparsity": 0.2579914023670934,
            "magnitude": 0.007483230974855692,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.160774329743295,
            "activation_std": 0.10209137883063094,
            "sparsity": 0.12447694312503352,
            "magnitude": 2.3094282145890976,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.3175560028252513,
            "head_specialization": 0.5786279064150414,
            "reasoning_score": 0.4323180376801819
          },
          "16": {
            "entropy": 2.1046566396859,
            "head_specialization": 0.6372864559961561,
            "reasoning_score": 0.43167738270805595
          },
          "24": {
            "entropy": 1.0148425975642144,
            "head_specialization": 0.4638954399062418,
            "reasoning_score": 0.29850579785843473
          },
          "31": {
            "entropy": 1.194324939660111,
            "head_specialization": 0.3697823533640632,
            "reasoning_score": 0.5989226279881885
          }
        }
      },
      {
        "sample": 9,
        "question": "Bryan took a look at his books as well. If he has 34 books distributed equally in 2 bookshelves How ...",
        "expected": "17",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.23308113233900787,
            "activation_std": 0.06384509506468632,
            "sparsity": 0.1933655246675839,
            "magnitude": 2.714863450175508,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.15308657224954395,
            "activation_std": 0.5354328588286031,
            "sparsity": 0.3829939689217269,
            "magnitude": 0.8634425880601236,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.10551768131660962,
            "activation_std": 0.37645345902030397,
            "sparsity": 0.1892972365932125,
            "magnitude": 0.12614281485409706,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.03799345756298812,
            "activation_std": 0.022638311756134982,
            "sparsity": 0.30951836897191887,
            "magnitude": 0.6458796917005366,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.04699165654800257,
            "activation_std": 0.022051109557546168,
            "sparsity": 0.28293244457328687,
            "magnitude": 0.12376149417010711,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 0.8710391887219779,
            "head_specialization": 0.46764688149281525,
            "reasoning_score": 0.38900480632273093
          },
          "16": {
            "entropy": 1.1519328767096986,
            "head_specialization": 0.3530113543989463,
            "reasoning_score": 0.32210676212060607
          },
          "24": {
            "entropy": 3.7575424037305183,
            "head_specialization": 0.40185305002283256,
            "reasoning_score": 0.4219699961307269
          },
          "31": {
            "entropy": 2.0476093400205624,
            "head_specialization": 0.2834415823840453,
            "reasoning_score": 0.525668630777136
          }
        }
      },
      {
        "sample": 10,
        "question": "Dave had 15 apps on his phone. He added 71 new apps. After deleting some he had 14 left. How many mo...",
        "expected": "1",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.1414755313661176,
            "activation_std": 0.2541957158001737,
            "sparsity": 0.21526407538506542,
            "magnitude": 1.649538812898322,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.17608579130560206,
            "activation_std": 0.15491005867034874,
            "sparsity": 0.20703559803153015,
            "magnitude": 0.7010388939721639,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.12326409337881561,
            "activation_std": 0.1425631474357489,
            "sparsity": 0.5611815092123217,
            "magnitude": 1.0563230418293486,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.11777342577919146,
            "activation_std": 0.21664348235495806,
            "sparsity": 0.41114286453562154,
            "magnitude": 1.448542835940405,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.022733440929326134,
            "activation_std": 0.050479914945375265,
            "sparsity": 0.05025266037719456,
            "magnitude": 3.1217194444473253,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.215028118714657,
            "head_specialization": 0.4562318139673014,
            "reasoning_score": 0.464268940451209
          },
          "16": {
            "entropy": 1.7060598369571924,
            "head_specialization": 0.37576480944682816,
            "reasoning_score": 0.44330535298859414
          },
          "24": {
            "entropy": 0.5861103026308592,
            "head_specialization": 0.31708907765166344,
            "reasoning_score": 0.21450197952212222
          },
          "31": {
            "entropy": 2.0487917629491235,
            "head_specialization": 0.4589629308871366,
            "reasoning_score": 0.40282597274613247
          }
        }
      },
      {
        "sample": 11,
        "question": "Adam has 4 more apples than Jackie. Together Adam and Jackie have 14 apples. Bob has 6 apples more t...",
        "expected": "20",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.15491029710486248,
            "activation_std": 0.27569407179634436,
            "sparsity": 0.6604141070951702,
            "magnitude": 1.410712109911276,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.12191390811955698,
            "activation_std": 0.22440670144674926,
            "sparsity": 0.23673895256380645,
            "magnitude": 1.4107729770936697,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.17307159177735731,
            "activation_std": 0.13973821490218435,
            "sparsity": 0.3994758546002387,
            "magnitude": 1.9272014930816213,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.08018470099628847,
            "activation_std": 0.03013490456774929,
            "sparsity": 0.30292382796267464,
            "magnitude": 0.16962813537969204,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.09310201931822194,
            "activation_std": 0.21077408086208047,
            "sparsity": 0.4365619090053769,
            "magnitude": 0.7152227081089992,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 4.481751589619123,
            "head_specialization": 0.4509058533529103,
            "reasoning_score": 0.38984835139930335
          },
          "16": {
            "entropy": 0.544077697974267,
            "head_specialization": 0.4508092032324122,
            "reasoning_score": 0.5553164462866499
          },
          "24": {
            "entropy": 1.1882997827789559,
            "head_specialization": 0.6419083813839023,
            "reasoning_score": 0.35202605010723226
          },
          "31": {
            "entropy": 1.052829568202219,
            "head_specialization": 0.6093537565686918,
            "reasoning_score": 0.5058164193906524
          }
        }
      },
      {
        "sample": 12,
        "question": "You have 104 dollars. How many packs of dvds can you buy if each pack costs 26 dollars?",
        "expected": "4",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.07419762657247597,
            "activation_std": 0.038933996923401874,
            "sparsity": 0.0820567816650189,
            "magnitude": 1.099540618876997,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.0027549666707492233,
            "activation_std": 0.1903464080985065,
            "sparsity": 0.2983199771616498,
            "magnitude": 1.8818981981508118,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.0027688352137811793,
            "activation_std": 0.24173166337670374,
            "sparsity": 0.17254916713336416,
            "magnitude": 0.18155848831333868,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.16725985424325363,
            "activation_std": 0.1012825590580356,
            "sparsity": 0.2623593479415427,
            "magnitude": 0.2561568616760822,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.055344514988391484,
            "activation_std": 0.14684232195885485,
            "sparsity": 0.8012780988926986,
            "magnitude": 2.1690377347740557,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.5348162353258257,
            "head_specialization": 0.6134042613804805,
            "reasoning_score": 0.37778542213565697
          },
          "16": {
            "entropy": 1.7261316125945454,
            "head_specialization": 0.5506716992674537,
            "reasoning_score": 0.5740742180499441
          },
          "24": {
            "entropy": 2.0013107384182107,
            "head_specialization": 0.35084671684664176,
            "reasoning_score": 0.2530069068363632
          },
          "31": {
            "entropy": 0.5232685335417551,
            "head_specialization": 0.5061454500978819,
            "reasoning_score": 0.28706324847621656
          }
        }
      },
      {
        "sample": 13,
        "question": "Jerry had 7 books and 3 action figures on a shelf in his room. Later he added 2 more action figures ...",
        "expected": "2",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.04952371347000681,
            "activation_std": 0.09717029089862154,
            "sparsity": 0.1456584424097595,
            "magnitude": 0.1430856652269316,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.09133690863640578,
            "activation_std": 0.054153839064144504,
            "sparsity": 0.169526372600138,
            "magnitude": 0.08722617860992166,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.027093662902348856,
            "activation_std": 0.15819419648119964,
            "sparsity": 0.07606249590659166,
            "magnitude": 1.4601371662654958,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.18259795570878976,
            "activation_std": 0.25552111732548466,
            "sparsity": 0.2727708559979713,
            "magnitude": 0.1236521226099813,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.07524456605876911,
            "activation_std": 0.12559075654796914,
            "sparsity": 0.44637693986990346,
            "magnitude": 0.2784547427924714,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.141502111392017,
            "head_specialization": 0.44826495654513615,
            "reasoning_score": 0.4823018763795262
          },
          "16": {
            "entropy": 4.06926018967995,
            "head_specialization": 0.4781841713192352,
            "reasoning_score": 0.5313670138743535
          },
          "24": {
            "entropy": 2.3064114569120933,
            "head_specialization": 0.6326181647516752,
            "reasoning_score": 0.3067663976404364
          },
          "31": {
            "entropy": 1.3954914301273509,
            "head_specialization": 0.5837656465695934,
            "reasoning_score": 0.26736828611369884
          }
        }
      },
      {
        "sample": 14,
        "question": "Melissa scored a total of 91 points in 13 games scoring the same for each game. How many points did ...",
        "expected": "7",
        "generated": "The answer is A",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.10021743992329639,
            "activation_std": 0.060225268286331915,
            "sparsity": 0.49850134798520296,
            "magnitude": 1.6649613976193416,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.3032140657364182,
            "activation_std": 0.22639959851805308,
            "sparsity": 0.06159640875705825,
            "magnitude": 1.5860135675998976,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.0915402250297138,
            "activation_std": 0.05558989740406314,
            "sparsity": 0.43214847764541675,
            "magnitude": 0.7583028269951603,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.0882521984202721,
            "activation_std": 0.07141664384314496,
            "sparsity": 0.22448135024180926,
            "magnitude": 0.649886374847584,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.014664360718154863,
            "activation_std": 0.5148703700734991,
            "sparsity": 0.424490333653623,
            "magnitude": 0.7846606554991272,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.07636782501748,
            "head_specialization": 0.41222983015179243,
            "reasoning_score": 0.3799255896638635
          },
          "16": {
            "entropy": 3.1278746930540584,
            "head_specialization": 0.6503504734017017,
            "reasoning_score": 0.3833213496401245
          },
          "24": {
            "entropy": 1.217083623206723,
            "head_specialization": 0.3828664657020994,
            "reasoning_score": 0.6424665718783726
          },
          "31": {
            "entropy": 4.5888018327097315,
            "head_specialization": 0.45860643276572594,
            "reasoning_score": 0.4673231706827122
          }
        }
      },
      {
        "sample": 15,
        "question": "Faye had 35 packs of pencils each one having 4 pencils. She was placing her pencils into rows with 2...",
        "expected": "70",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.04793994082314774,
            "activation_std": 0.30827915661787597,
            "sparsity": 0.332320276101175,
            "magnitude": 1.49109177682832,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.1112308407951078,
            "activation_std": 0.33706137482064547,
            "sparsity": 0.1127968306120371,
            "magnitude": 0.1946941282110639,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.18526651435002447,
            "activation_std": 0.14776183519220665,
            "sparsity": 0.11797795406043589,
            "magnitude": 2.329671452508416,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.01570122107768888,
            "activation_std": 0.03712815646686105,
            "sparsity": 0.2653650410079023,
            "magnitude": 0.5703607870133882,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.04485797154832633,
            "activation_std": 0.13937048129190618,
            "sparsity": 0.12259743939527246,
            "magnitude": 4.485594520910181,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 2.1266967633255067,
            "head_specialization": 0.3319434064270401,
            "reasoning_score": 0.30929922722994757
          },
          "16": {
            "entropy": 1.3720208271235925,
            "head_specialization": 0.4187604115754573,
            "reasoning_score": 0.5247253267689865
          },
          "24": {
            "entropy": 1.8838027471072962,
            "head_specialization": 0.32987012247763253,
            "reasoning_score": 0.27047440716385424
          },
          "31": {
            "entropy": 2.7768188974776224,
            "head_specialization": 0.601106287107208,
            "reasoning_score": 0.5017431695248301
          }
        }
      },
      {
        "sample": 16,
        "question": "Each basket of peaches has 19 red peaches and 4 green peaches. If there are 15 such baskets How many...",
        "expected": "345",
        "generated": "145",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.12845656152238424,
            "activation_std": 0.10945303875975596,
            "sparsity": 0.3560550932447162,
            "magnitude": 0.6981671428579928,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": 0.02147970248151603,
            "activation_std": 0.13269672706768607,
            "sparsity": 0.4235923660197835,
            "magnitude": 3.290642379302623,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.12085454397832132,
            "activation_std": 0.017042863492676107,
            "sparsity": 0.4696587872320026,
            "magnitude": 0.017599953031254466,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.012608732929330934,
            "activation_std": 0.17058277256515436,
            "sparsity": 0.14898250516836187,
            "magnitude": 1.2411734810602448,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": 0.08444866477366243,
            "activation_std": 0.10813451697248366,
            "sparsity": 0.4013485714873338,
            "magnitude": 4.6263118498553055,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 3.7384040981478597,
            "head_specialization": 0.5197074519313617,
            "reasoning_score": 0.5936668293706421
          },
          "16": {
            "entropy": 1.285448448470735,
            "head_specialization": 0.31398779179952857,
            "reasoning_score": 0.49384015885290716
          },
          "24": {
            "entropy": 2.3071796157951825,
            "head_specialization": 0.528359383360623,
            "reasoning_score": 0.3804351997526132
          },
          "31": {
            "entropy": 2.123307986102249,
            "head_specialization": 0.3165755452353067,
            "reasoning_score": 0.6000525225918547
          }
        }
      },
      {
        "sample": 17,
        "question": "Nell collects cards. She had 309 baseball cards and 356 Ace cards. She gave some of her cards to Jef...",
        "expected": "266",
        "generated": "The answer is A",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": -0.09437099232875867,
            "activation_std": 0.20561829700106635,
            "sparsity": 0.4763411107775619,
            "magnitude": 0.20978448421334528,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.09790888384088091,
            "activation_std": 0.3927636069556267,
            "sparsity": 0.16374014797640277,
            "magnitude": 0.6885679341221468,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": -0.06589476779459937,
            "activation_std": 0.08018171069991109,
            "sparsity": 0.19861552150988573,
            "magnitude": 0.43049809052847987,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": 0.14237076082845362,
            "activation_std": 0.0386688320325151,
            "sparsity": 0.18130878474286793,
            "magnitude": 0.20734459714056683,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.030951889080879827,
            "activation_std": 0.22176384399773522,
            "sparsity": 0.21755996451372112,
            "magnitude": 0.21691814957376004,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.132327233087242,
            "head_specialization": 0.24483127862474952,
            "reasoning_score": 0.40541696099602226
          },
          "16": {
            "entropy": 1.3251041853464496,
            "head_specialization": 0.3963171802886047,
            "reasoning_score": 0.5505493894980871
          },
          "24": {
            "entropy": 2.45262839848992,
            "head_specialization": 0.407011482929071,
            "reasoning_score": 0.2771664071232351
          },
          "31": {
            "entropy": 1.7442238674916648,
            "head_specialization": 0.6057011773364681,
            "reasoning_score": 0.4456236400461766
          }
        }
      },
      {
        "sample": 18,
        "question": "There are 270 students in a school. If the school has 5 students in each grades and each grade has t...",
        "expected": "54",
        "generated": "3",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.0324899427312348,
            "activation_std": 0.20440178360150182,
            "sparsity": 0.08748435535747393,
            "magnitude": 1.7393250815949248,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.13349155941490948,
            "activation_std": 0.1255207177291201,
            "sparsity": 0.20107015200011158,
            "magnitude": 1.546938808682164,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.2886546535488349,
            "activation_std": 0.05161332329777453,
            "sparsity": 0.3264935113842799,
            "magnitude": 0.10965289515215464,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.20745305023476504,
            "activation_std": 0.13934161041666546,
            "sparsity": 0.06278716829158337,
            "magnitude": 0.8047143906919294,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.016201317247162138,
            "activation_std": 0.13339148268093062,
            "sparsity": 0.1173858508994281,
            "magnitude": 0.22452701584469342,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.9982800156618634,
            "head_specialization": 0.41278030801369003,
            "reasoning_score": 0.5229509152405433
          },
          "16": {
            "entropy": 1.4391948852116663,
            "head_specialization": 0.48594234244104656,
            "reasoning_score": 0.490443984975696
          },
          "24": {
            "entropy": 2.8789846522684717,
            "head_specialization": 0.3608501717183605,
            "reasoning_score": 0.41899570545737125
          },
          "31": {
            "entropy": 2.173447430166198,
            "head_specialization": 0.4229824166810836,
            "reasoning_score": 0.47539398072854905
          }
        }
      },
      {
        "sample": 19,
        "question": "In a school there are 34 girls and 841 boys. How many more boys than girls does the school have?",
        "expected": "807",
        "generated": "19",
        "correct": false,
        "layer_analysis": {
          "0": {
            "activation_mean": 0.002478011239610803,
            "activation_std": 0.06114462267387869,
            "sparsity": 0.3842288336287984,
            "magnitude": 3.4076389611425726,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "8": {
            "activation_mean": -0.15793015487732004,
            "activation_std": 0.029524564550224022,
            "sparsity": 0.10589251965758523,
            "magnitude": 0.3718692057078953,
            "quantization_bits": 2,
            "critical_layer": false
          },
          "16": {
            "activation_mean": 0.47282940350602387,
            "activation_std": 0.06570993040940432,
            "sparsity": 0.290475863240304,
            "magnitude": 0.9658992694800705,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "24": {
            "activation_mean": -0.2786394601515012,
            "activation_std": 0.1329008535362917,
            "sparsity": 0.14376812831109545,
            "magnitude": 0.36829142026339184,
            "quantization_bits": 2,
            "critical_layer": true
          },
          "31": {
            "activation_mean": -0.032975570746009486,
            "activation_std": 0.18872650618637032,
            "sparsity": 0.21791422587405618,
            "magnitude": 0.3645898333580514,
            "quantization_bits": 2,
            "critical_layer": true
          }
        },
        "attention_analysis": {
          "8": {
            "entropy": 1.9550668174011525,
            "head_specialization": 0.3303443305326623,
            "reasoning_score": 0.4101693307284004
          },
          "16": {
            "entropy": 2.850634338574155,
            "head_specialization": 0.2858254423832135,
            "reasoning_score": 0.5169914858853862
          },
          "24": {
            "entropy": 1.4718800754354338,
            "head_specialization": 0.4581055638884202,
            "reasoning_score": 0.4593494824857101
          },
          "31": {
            "entropy": 1.1009945854961156,
            "head_specialization": 0.5392703630789414,
            "reasoning_score": 0.1323505334783506
          }
        }
      }
    ]
  }
}